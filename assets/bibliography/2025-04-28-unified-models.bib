@article{gregor2015draw,
  title={DRAW: A recurrent neural network for image generation},
  author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint, arXiv:1502.04623},
  year={2015},
  url={https://arxiv.org/pdf/1502.04623.pdf}
}


@article{chameleon2024,
  author    = {Team, Chameleon},
  title     = {Chameleon: Mixed-modal early-fusion foundation models},
  journal   = {arXiv preprint},
  volume    = {arXiv:2405.09818},
  year      = {2024},
  url       = {https://arxiv.org/abs/2405.09818}
}


@article{xie2024showo,
  title={Show-o: One single transformer to unify multimodal understanding and generation},
  author={Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2408.12528},
  year={2024}
}


@article{yang2024mmar,
  title={MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling},
  author={Yang, Jian and Yin, Dacheng and Zhou, Yizhou and Rao, Fengyun and Zhai, Wei and Cao, Yang and Zha, Zheng-Jun},
  journal={arXiv preprint arXiv:2410.10798},
  year={2024}
}


@article{zhou2024transfusion,
  title={Transfusion: Predict the next token and diffuse images with one multi-modal model},
  author={Zhou, Chunting and Yu, Lili and Babu, Arun and Tirumala, Kushal and Yasunaga, Michihiro and Shamis, Leonid and Kahn, Jacob and Ma, Xuezhe and Zettlemoyer, Luke and Levy, Omer},
  journal={arXiv preprint arXiv:2408.11039},
  year={2024}
}


@article{wang2024emu3,
  title={Emu3: Next-token prediction is all you need},
  author={Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and others},
  journal={arXiv preprint arXiv:2409.18869},
  year={2024}
}


@article{wu2024vila,
  title={Vila-u: a unified foundation model integrating visual understanding and generation},
  author={Wu, Yecheng and Zhang, Zhuoyang and Chen, Junyu and Tang, Haotian and Li, Dacheng and Fang, Yunhao and Zhu, Ligeng and Xie, Enze and Yin, Hongxu and Yi, Li and others},
  journal={arXiv preprint arXiv:2409.04429},
  year={2024}
}


@article{liu2024world,
  title={World model on million-length video and language with ringattention},
  author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2402.08268},
  year={2024}
}


@article{zhao2024monoformer,
  title={Monoformer: One transformer for both diffusion and autoregression},
  author={Zhao, Chuyang and Song, Yuxing and Wang, Wenhao and Feng, Haocheng and Ding, Errui and Sun, Yifan and Xiao, Xinyan and Wang, Jingdong},
  journal={arXiv preprint arXiv:2409.16280},
  year={2024}
}


@article{ge2024seed,
  title={Seed-x: Multimodal models with unified multi-granularity comprehension and generation},
  author={Ge, Yuying and Zhao, Sijie and Zhu, Jinguo and Ge, Yixiao and Yi, Kun and Song, Lin and Li, Chen and Ding, Xiaohan and Shan, Ying},
  journal={arXiv preprint arXiv:2404.14396},
  year={2024}
}

@inproceedings{jin2024unified,
  title={Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization},
  author={Jin, Yang and Xu, Kun and Xu, Kun and Chen, Liwei and Liao, Chao and Tan, Jianchao and Mu, Yadong and others},
  booktitle={International Conference on Learning Representations},
  year={2024}
}


@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
