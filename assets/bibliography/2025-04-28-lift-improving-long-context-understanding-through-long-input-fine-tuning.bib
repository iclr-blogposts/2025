@misc{AllenZhu-icml2024-tutorial,
    author = {Allen-Zhu, Zeyuan},
    title = {ICML 2024 Tutorial: Physics of Language Models},
    year = {2024},
    month = {July},
    note = {Project page: \url{https://physics.allen-zhu.com/}}
}
@article{llama3modelcard,
    title={Llama 3 Model Card},
    author={AI@Meta},
    year={2024},
    url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}
@article{chen2023robust,
  title={How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks},
  author={Chen, Xuanting and Ye, Junjie and Zu, Can and Xu, Nuo and Zheng, Rui and Peng, Minlong and Zhou, Jie and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2303.00293},
  year={2023}
}

@article{lu2024controlled,
  title={A Controlled Study on Long Context Extension and Generalization in LLMs},
  author={Lu, Yi and Yan, Jing Nathan and Yang, Songlin and Chiu, Justin T and Ren, Siyu and Yuan, Fei and Zhao, Wenting and Wu, Zhiyong and Rush, Alexander M},
  journal={arXiv preprint arXiv:2409.12181},
  year={2024}
}
@article{wang2024beyond,
  title={Beyond the limits: A survey of techniques to extend the context length in large language models},
  author={Wang, Xindi and Salmani, Mahsa and Omidi, Parsa and Ren, Xiangyu and Rezagholizadeh, Mehdi and Eshaghi, Armaghan},
  journal={arXiv preprint arXiv:2402.02244},
  year={2024}
}
@article{huang2023advancing,
  title={Advancing transformer architecture in long-context large language models: A comprehensive survey},
  author={Huang, Yunpeng and Xu, Jingwei and Jiang, Zixu and Lai, Junyu and Li, Zenan and Yao, Yuan and Chen, Taolue and Yang, Lijuan and Xin, Zhou and Ma, Xiaoxing},
  journal={arXiv preprint arXiv:2311.12351},
  year={2023}
}
@article{xiong2023effective,
  title={Effective long-context scaling of foundation models},
  author={Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others},
  journal={arXiv preprint arXiv:2309.16039},
  year={2023}
}
@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}
@article{xu2023retrieval,
  title={Retrieval meets long context large language models},
  author={Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2310.03025},
  year={2023}
}
@article{jiang2024longrag,
  title={Longrag: Enhancing retrieval-augmented generation with long-context llms},
  author={Jiang, Ziyan and Ma, Xueguang and Chen, Wenhu},
  journal={arXiv preprint arXiv:2406.15319},
  year={2024}
}
@article{chen2023longlora,
  title={Longlora: Efficient fine-tuning of long-context large language models},
  author={Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
  journal={arXiv preprint arXiv:2309.12307},
  year={2023}
}
@article{peng2023yarn,
  title={Yarn: Efficient context window extension of large language models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  journal={arXiv preprint arXiv:2309.00071},
  year={2023}
}
@article{li2023loogle,
  title={LooGLE: Can Long-Context Language Models Understand Long Contexts?},
  author={Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan},
  journal={arXiv preprint arXiv:2311.04939},
  year={2023}
}
@article{bai2023longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}
@article{dong2023bamboo,
  title={Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models},
  author={Dong, Zican and Tang, Tianyi and Li, Junyi and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2309.13345},
  year={2023}
}
@article{pang2021quality,
  title={QuALITY: Question answering with long input texts, yes!},
  author={Pang, Richard Yuanzhe and Parrish, Alicia and Joshi, Nitish and Nangia, Nikita and Phang, Jason and Chen, Angelica and Padmakumar, Vishakh and Ma, Johnny and Thompson, Jana and He, He and others},
  journal={arXiv preprint arXiv:2112.08608},
  year={2021}
}
@misc{niah,
  author = {Gregory Kamradt},
  title = {LLMTest_NeedleInAHaystack},
  year = {2023},
  url = {https://github.com/gkamradt/LLMTest_NeedleInAHaystack/blob/main/README.md},
  note = {Accessed: YYYY-MM-DD}
}
@misc{beltagy2020longformerlongdocumenttransformer,
  title={Longformer: The Long-Document Transformer}, 
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  year={2020},
  eprint={2004.05150},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2004.05150}, 
}
@misc{zaheer2021bigbirdtransformerslonger,
  title={Big Bird: Transformers for Longer Sequences}, 
  author={Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
  year={2021},
  eprint={2007.14062},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2007.14062}, 
}
@misc{kočiský2017narrativeqareadingcomprehensionchallenge,
  title={The NarrativeQA Reading Comprehension Challenge}, 
  author={Tomáš Kočiský and Jonathan Schwarz and Phil Blunsom and Chris Dyer and Karl Moritz Hermann and Gábor Melis and Edward Grefenstette},
  year={2017},
  eprint={1712.07040},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1712.07040}, 
}
@misc{feng2021sequencetosequenceapproachdialoguestate,
  title={A Sequence-to-Sequence Approach to Dialogue State Tracking}, 
  author={Yue Feng and Yang Wang and Hang Li},
  year={2021},
  eprint={2011.09553},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2011.09553}, 
}
@misc{tapaswi2016movieqaunderstandingstoriesmovies,
  title={MovieQA: Understanding Stories in Movies through Question-Answering}, 
  author={Makarand Tapaswi and Yukun Zhu and Rainer Stiefelhagen and Antonio Torralba and Raquel Urtasun and Sanja Fidler},
  year={2016},
  eprint={1512.02902},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/1512.02902}, 
}
@misc{zhang2024longclipunlockinglongtextcapability,
  title={Long-CLIP: Unlocking the Long-Text Capability of CLIP}, 
  author={Beichen Zhang and Pan Zhang and Xiaoyi Dong and Yuhang Zang and Jiaqi Wang},
  year={2024},
  eprint={2403.15378},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2403.15378}, 
}
@misc{wu2024longmemevalbenchmarkingchatassistants,
  title={LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory}, 
  author={Di Wu and Hongwei Wang and Wenhao Yu and Yuwei Zhang and Kai-Wei Chang and Dong Yu},
  year={2024},
  eprint={2410.10813},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2410.10813}, 
}
@misc{cho2024spectrumspeakerenhancedpretraininglong,
  title={SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization}, 
  author={Sangwoo Cho and Kaiqiang Song and Chao Zhao and Xiaoyang Wang and Dong Yu},
  year={2024},
  eprint={2401.17597},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2401.17597}, 
}