@article{Gou2021KDSurvey,
  title={Knowledge Distillation: A Survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume = 129,
  year={2021},
  pages = {1789--1819},
  url={https://doi.org/10.1007/s11263-021-01453-z}
}

@misc{Alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{Vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{Sanh2020DistilBERT,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108},
  url={https://api.semanticscholar.org/CorpusID:203626972}
}

@inproceedings{Wang2020MiniLM,
author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
title = {MINILM: deep self-attention distillation for task-agnostic compression of pre-trained transformers},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {485},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inbook{Malinin2019ReverseKL,
author = {Malinin, Andrey and Gales, Mark},
title = {Reverse KL-divergence training of prior networks: improved uncertainty and adversarial robustness},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Ensemble approaches for uncertainty estimation have recently been applied to the tasks of misclassification detection, out-of-distribution input detection and adversarial attack detection. Prior Networks have been proposed as an approach to efficiently emulate an ensemble of models for classification by parameterising a Dirichlet prior distribution over output distributions. These models have been shown to outperform alternative ensemble approaches, such as Monte-Carlo Dropout, on the task of out-of-distribution input detection. However, scaling Prior Networks to complex datasets with many classes is difficult using the training criteria originally proposed. This paper makes two contributions. First, we show that the appropriate training criterion for Prior Networks is the reverse KL-divergence between Dirichlet distributions. This addresses issues in the nature of the training data target distributions, enabling prior networks to be successfully trained on classification tasks with arbitrarily many classes, as well as improving out-of-distribution detection performance. Second, taking advantage of this new training criterion, this paper investigates using Prior Networks to detect adversarial attacks and proposes a generalized form of adversarial training. It is shown that the construction of successful adaptive whitebox attacks, which affect the prediction and evade detection, against Prior Networks trained on CIFAR-10 and CIFAR-100 using the proposed approach requires a greater amount of computational effort than against networks defended using standard adversarial training or MC-dropout.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1303},
numpages = {12}
}

@Article{Williams1992PolicyGradient,
author={Williams, Ronald J.},
title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
journal={Machine Learning},
year={1992},
month={May},
day={01},
volume={8},
number={3},
pages={229-256},
issn={1573-0565},
doi={10.1007/BF00992696},
url={https://doi.org/10.1007/BF00992696}
}

@inproceedings{Skalse2024RewardHacking,
author = {Skalse, Joar and Howe, Nikolaus H. R. and Krasheninnikov, Dmitrii and Krueger, David},
title = {Defining and characterizing reward hacking},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {687},
numpages = {12},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}
