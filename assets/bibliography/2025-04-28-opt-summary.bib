@book{nesterov2018lectures,
  title={Lectures on Convex Optimization},
  author={Nesterov, Yurii},
  volume={137},
  year={2018},
  publisher={Springer},
  url={https://link.springer.com/book/10.1007/978-3-319-91578-4}
}

@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.},
  url={https://arxiv.org/abs/1405.4980}
}

@book{blair1985problem,
	author = {Nemirovski, Arkadi. S. and  Yudin, David. B.},
	title = {{Problem Complexity and Method Efficiency in Optimization}},
	publisher = {Wiley, New York},
	year = {1983},
  url={https://www2.isye.gatech.edu/~nemirovs/Nemirovskii_Yudin_1983.pdf}
}

@article{carmon2021lower,
  title={Lower bounds for finding stationary points II: first-order methods},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={Mathematical Programming},
  volume={185},
  number={1},
  pages={315--355},
  year={2021},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-019-01431-x}
}
@article{carmon2020lower,
  title={Lower bounds for finding stationary points I},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={Mathematical Programming},
  volume={184},
  number={1},
  pages={71--120},
  year={2020},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-019-01406-y}
}

@article{davis2018stochastic,
  title={Stochastic subgradient method converges at the rate $ O (k^{-1/4}) $ on weakly convex functions},
  author={Davis, Damek and Drusvyatskiy, Dmitriy},
  journal={arXiv preprint arXiv:1802.02988},
  year={2018},
  url={https://arxiv.org/abs/1802.02988}
}

@article{defazio2016simple,
  title={A simple practical accelerated method for finite sums},
  author={Defazio, Aaron},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016},
  url={https://proceedings.neurips.cc/paper_files/paper/2016/hash/4f6ffe13a5d75b2d6a3923922b3922e5-Abstract.html}
}

@article{allen2018katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={221},
  pages={1--51},
  year={2018},
  url={https://www.jmlr.org/papers/v18/16-410.html}
}

@article{woodworth2016tight,
  title={Tight complexity bounds for optimizing composite objectives},
  author={Woodworth, Blake E and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016},
  url={https://proceedings.neurips.cc/paper/2016/hash/645098b086d2f9e1e0e939c27f9f2d6f-Abstract.html}
}

@inproceedings{allen2018katyusha,
  title={Katyusha x: Simple momentum method for stochastic sum-of-nonconvex optimization},
  author={Allen-Zhu, Zeyuan},
  booktitle={International Conference on Machine Learning},
  pages={179--185},
  year={2018},
  organization={PMLR},
  url={https://proceedings.mlr.press/v80/allen-zhu18a.html}
}

@article{xie2019general,
  title={A general analysis framework of lower complexity bounds for finite-sum optimization},
  author={Xie, Guangzeng and Luo, Luo and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1908.08394},
  year={2019},
  url={https://arxiv.org/abs/1908.08394}
}

@inproceedings{zhou2019lower,
  title={Lower bounds for smooth nonconvex finite-sum optimization},
  author={Zhou, Dongruo and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={7574--7583},
  year={2019},
  organization={PMLR},
  url={https://proceedings.mlr.press/v97/zhou19b.html}
}

@article{wang2019spiderboost,
  title={Spiderboost and momentum: Faster variance reduction algorithms},
  author={Wang, Zhe and Ji, Kaiyi and Zhou, Yi and Liang, Yingbin and Tarokh, Vahid},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  url={https://proceedings.neurips.cc/paper/2019/hash/512c5cad6c37edb98ae91c8a76c3a291-Abstract.html}
}

@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018},
  url={https://proceedings.neurips.cc/paper_files/paper/2018/hash/1543843a4723ed2ab08e18053ae6dc5b-Abstract.html}
}

@article{ghadimi2012optimal,
  title={Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={4},
  pages={1469--1492},
  year={2012},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/abs/10.1137/110848864}
}

@inproceedings{rakhlin2012making,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  booktitle={Proceedings of the 29th International Coference on International Conference on Machine Learning},
  pages={1571--1578},
  year={2012},
  url={https://icml.cc/2012/papers/261.pdf}
}

@article{lan2012optimal,
  title={An optimal method for stochastic composite optimization},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  volume={133},
  number={1},
  pages={365--397},
  year={2012},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-010-0434-y}
}

@article{woodworth2018graph,
  title={Graph oracle models, lower bounds, and gaps for parallel stochastic optimization},
  author={Woodworth, Blake E and Wang, Jialei and Smith, Adam and McMahan, Brendan and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018},
  url={https://proceedings.neurips.cc/paper_files/paper/2018/hash/3ec27c2cff04bc5fd2586ca36c62044e-Abstract.html}
}

@article{hazan2014beyond,
  title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization},
  author={Hazan, Elad and Kale, Satyen},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2489--2512},
  year={2014},
  url={https://www.jmlr.org/papers/volume15/hazan14a/hazan14a.pdf}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/abs/10.1137/070704277}
}

@article{agarwal2009information,
  title={Information-theoretic lower bounds on the oracle complexity of convex optimization},
  author={Agarwal, Alekh and Wainwright, Martin J and Bartlett, Peter and Ravikumar, Pradeep},
  journal={Advances in Neural Information Processing Systems},
  volume={22},
  year={2009},
  url={https://proceedings.neurips.cc/paper/2009/hash/2387337ba1e0b0249ba90f55b2ba2521-Abstract.html}
}

@inproceedings{foster2019complexity,
  title={The complexity of making the gradient small in stochastic convex optimization},
  author={Foster, Dylan J and Sekhari, Ayush and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik and Woodworth, Blake},
  booktitle={Conference on Learning Theory},
  pages={1319--1345},
  year={2019},
  organization={PMLR},
  url={https://proceedings.mlr.press/v99/foster19b.html}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM journal on optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/abs/10.1137/120880811}
}

@article{arjevani2023lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={Mathematical Programming},
  volume={199},
  number={1},
  pages={165--214},
  year={2023},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-022-01822-7}
}

@article{chambolle2011first,
  title={A first-order primal-dual algorithm for convex problems with applications to imaging},
  author={Chambolle, Antonin and Pock, Thomas},
  journal={Journal of mathematical imaging and vision},
  volume={40},
  pages={120--145},
  year={2011},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10851-010-0251-1}
}

@article{ouyang2021lower,
  title={Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems},
  author={Ouyang, Yuyuan and Xu, Yangyang},
  journal={Mathematical Programming},
  volume={185},
  number={1},
  pages={1--35},
  year={2021},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-019-01420-0}
}

@article{nemirovski2004prox,
  title={Prox-method with rate of convergence O (1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems},
  author={Nemirovski, Arkadi},
  journal={SIAM Journal on Optimization},
  volume={15},
  number={1},
  pages={229--251},
  year={2004},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/abs/10.1137/S1052623403425629}
}

@article{mokhtari2020convergence,
  title={Convergence rate of O(1/k) for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems},
  author={Mokhtari, Aryan and Ozdaglar, Asuman E and Pattathil, Sarath},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={4},
  pages={3230--3251},
  year={2020},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/abs/10.1137/19M127375X}
}

@inproceedings{xie2020lower,
  title={Lower complexity bounds for finite-sum convex-concave minimax optimization problems},
  author={Xie, Guangzeng and Luo, Luo and Lian, Yijiang and Zhang, Zhihua},
  booktitle={International Conference on Machine Learning},
  pages={10504--10513},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v119/xie20d.html}
}

@article{yang2020catalyst,
  title={A catalyst framework for minimax optimization},
  author={Yang, Junchi and Zhang, Siqi and Kiyavash, Negar and He, Niao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5667--5678},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/hash/3db54f5573cd617a0112d35dd1e6b1ef-Abstract.html}
}

@inproceedings{du2019linear,
  title={Linear convergence of the primal-dual gradient method for convex-concave saddle point problems without strong convexity},
  author={Du, Simon S and Hu, Wei},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={196--205},
  year={2019},
  organization={PMLR},
  url={https://proceedings.mlr.press/v89/du19b.html}
}

@article{chambolle2016ergodic,
  title={On the ergodic convergence rates of a first-order primal--dual algorithm},
  author={Chambolle, Antonin and Pock, Thomas},
  journal={Mathematical Programming},
  volume={159},
  number={1},
  pages={253--287},
  year={2016},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-015-0957-3}
}

@article{zhang2022lower,
  title={On lower iteration complexity bounds for the convex concave saddle point problems},
  author={Zhang, Junyu and Hong, Mingyi and Zhang, Shuzhong},
  journal={Mathematical Programming},
  volume={194},
  number={1},
  pages={901--935},
  year={2022},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10107-021-01660-z}
}

@article{wang2020improved,
  title={Improved algorithms for convex-concave minimax optimization},
  author={Wang, Yuanhao and Li, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4800--4810},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/hash/331316d4efb44682092a006307b9ae3a-Abstract.html}
}

@inproceedings{liang2019interaction,
  title={Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks},
  author={Liang, Tengyuan and Stokes, James},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={907--915},
  year={2019},
  organization={PMLR},
  url={https://proceedings.mlr.press/v89/liang19b.html}
}

@inproceedings{ibrahim2020linear,
  title={Linear lower bounds and conditioning of differentiable games},
  author={Ibrahim, Adam and Azizian, Wa{\i}ss and Gidel, Gauthier and Mitliagkas, Ioannis},
  booktitle={International conference on machine learning},
  pages={4583--4593},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v119/ibrahim20a.html}
}

@article{arjevani2016lower,
  title={On lower and upper bounds in smooth and strongly convex optimization},
  author={Arjevani, Yossi and Shalev-Shwartz, Shai and Shamir, Ohad},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={126},
  pages={1--51},
  year={2016},
  url={https://www.jmlr.org/papers/v17/15-106.html}
}

@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016},
  url={https://arxiv.org/abs/1609.04747}
}

@article{yazdandoost2023stochastic,
  title={A stochastic variance-reduced accelerated primal-dual method for finite-sum saddle-point problems},
  author={Yazdandoost Hamedani, Erfan and Jalilzadeh, Afrooz},
  journal={Computational Optimization and Applications},
  volume={85},
  number={2},
  pages={653--679},
  year={2023},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10589-023-00472-5}
}

@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS},
  url={https://pubsonline.informs.org/doi/abs/10.1287/10-SSY011}
}

@article{palaniappan2016stochastic,
  title={Stochastic variance reduction methods for saddle-point problems},
  author={Palaniappan, Balamurugan and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016},
  url={https://proceedings.neurips.cc/paper_files/paper/2016/hash/1aa48fc4880bb0c9b8a3bf979d3b917e-Abstract.html}
}

@article{hsieh2019convergence,
  title={On the convergence of single-call stochastic extra-gradient methods},
  author={Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Mertikopoulos, Panayotis},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  url={https://proceedings.neurips.cc/paper/2019/hash/4625d8e31dad7d1c4c83399a6eb62f0c-Abstract.html}
}

@article{yan2020optimal,
  title={Optimal epoch stochastic gradient descent ascent methods for min-max optimization},
  author={Yan, Yan and Xu, Yi and Lin, Qihang and Liu, Wei and Yang, Tianbao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5789--5800},
  year={2020},
  url={https://proceedings.neurips.cc/paper_files/paper/2020/hash/3f8b2a81da929223ae025fcec26dde0d-Abstract.html}
}

@article{luo2019stochastic,
  title={A stochastic proximal point algorithm for saddle-point problems},
  author={Luo, Luo and Chen, Cheng and Li, Yujun and Xie, Guangzeng and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1909.06946},
  year={2019},
  url={https://arxiv.org/abs/1909.06946}
}

@article{chavdarova2019reducing,
  title={Reducing noise in GAN training with variance reduced extragradient},
  author={Chavdarova, Tatjana and Gidel, Gauthier and Fleuret, Fran{\c{c}}ois and Lacoste-Julien, Simon},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  url={https://proceedings.neurips.cc/paper/2019/hash/58a2fc6ed39fd083f55d4182bf88826d-Abstract.html}
}

@inproceedings{zhang2022beyond,
  title={Beyond Worst-Case Analysis in Stochastic Approximation: Moment Estimation Improves Instance Complexity},
  author={Zhang, Jingzhao and Lin, Hongzhou and Das, Subhro and Sra, Suvrit and Jadbabaie, Ali},
  booktitle={International Conference on Machine Learning},
  pages={26347--26361},
  year={2022},
  organization={PMLR},
  url={https://proceedings.mlr.press/v162/zhang22r}
}

@inproceedings{zhang2021complexity,
  title={The complexity of nonconvex-strongly-concave minimax optimization},
  author={Zhang, Siqi and Yang, Junchi and Guzm{\'a}n, Crist{\'o}bal and Kiyavash, Negar and He, Niao},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={482--492},
  year={2021},
  organization={PMLR},
  url={https://proceedings.mlr.press/v161/zhang21c.html}
}

@inproceedings{lin2020near,
  title={Near-optimal algorithms for minimax optimization},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2738--2779},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v125/lin20a.html}
}

@inproceedings{lin2020gradient,
  title={On gradient descent ascent for nonconvex-concave minimax problems},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={6083--6093},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v119/lin20a.html}
}

@article{boct2023alternating,
  title={Alternating proximal-gradient steps for (stochastic) nonconvex-concave minimax problems},
  author={Bo{\c{t}}, Radu Ioan and B{\"o}hm, Axel},
  journal={SIAM Journal on Optimization},
  volume={33},
  number={3},
  pages={1884--1913},
  year={2023},
  publisher={SIAM},
  url={https://epubs.siam.org/doi/full/10.1137/21M1465470}
}

@article{thekumparampil2019efficient,
  title={Efficient algorithms for smooth minimax optimization},
  author={Thekumparampil, Kiran K and Jain, Prateek and Netrapalli, Praneeth and Oh, Sewoong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  url={https://proceedings.neurips.cc/paper_files/paper/2019/hash/05d0abb9a864ae4981e933685b8b915c-Abstract.html}
}

@article{zhang2022sapd+,
  title={Sapd+: An accelerated stochastic method for nonconvex-concave minimax problems},
  author={Zhang, Xuan and Aybat, Necdet Serhat and Gurbuzbalaban, Mert},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21668--21681},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/hash/880d8999c07a8efc9bbbeb0c38f50765-Abstract-Conference.html}
}

@article{zhang2020single,
  title={A single-loop smoothed gradient descent-ascent algorithm for nonconvex-concave min-max problems},
  author={Zhang, Jiawei and Xiao, Peijun and Sun, Ruoyu and Luo, Zhiquan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7377--7389},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/hash/52aaa62e71f829d41d74892a18a11d59-Abstract.html}
}

@article{yang2020global,
  title={Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems},
  author={Yang, Junchi and Kiyavash, Negar and He, Niao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1153--1165},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/hash/0cc6928e741d75e7a92396317522069e-Abstract.html}
}

@inproceedings{yang2022faster,
  title={Faster single-loop algorithms for minimax optimization without strong concavity},
  author={Yang, Junchi and Orvieto, Antonio and Lucchi, Aurelien and He, Niao},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5485--5517},
  year={2022},
  organization={PMLR},
  url={https://proceedings.mlr.press/v151/yang22b.html}
}

@article{xu2023unified,
  title={A unified single-loop alternating gradient projection algorithm for nonconvex--concave and convex--nonconcave minimax problems},
  author={Xu, Zi and Zhang, Huiling and Xu, Yang and Lan, Guanghui},
  journal={Mathematical Programming},
  volume={201},
  number={1},
  pages={635--706},
  year={2023},
  publisher={Springer},
  url={,
  url={https://proceedings.mlr.press/v151/yang22b.html}}
}

@inproceedings{xu2019slides,
  title={(Slides) Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems},
  author={Xu, Yangyang},
  year={2019},
  url={https://xu-yangyang.github.io/slides/LowerBnd.pdf}
}

@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013},
  url={https://proceedings.neurips.cc/paper/2013/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf}
}

@article{garrigos2023handbook,
  title={Handbook of convergence theorems for (stochastic) gradient methods},
  author={Garrigos, Guillaume and Gower, Robert M},
  journal={arXiv preprint arXiv:2301.11235},
  year={2023},
  url={https://arxiv.org/abs/2301.11235}
}

@article{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskij, Arkadij Semenovi{\v{c}} and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley-Interscience}
}

@incollection{danilova2022recent,
  title={Recent theoretical advances in non-convex optimization},
  author={Danilova, Marina and Dvurechensky, Pavel and Gasnikov, Alexander and Gorbunov, Eduard and Guminov, Sergey and Kamzolov, Dmitry and Shibaev, Innokentiy},
  booktitle={High-Dimensional Optimization and Probability: With a View Towards Data Science},
  pages={79--163},
  year={2022},
  publisher={Springer},
  url={https://link.springer.com/chapter/10.1007/978-3-031-00832-0_3}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011},
  url={https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf}
}

@inproceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  booktitle={International Conference on Learning Representations},
  year={2015}
}

@inproceedings{boyd2024text,
  title={EE364a: Convex Optimization I},
  author={Boyd, Stephen},
  year={2024},
  url={https://web.stanford.edu/class/ee364a/}
}

@inproceedings{boyd2024video,
  title={Stanford EE364A Convex Optimization Course Video (Stanford Online)},
  author={Boyd, Stephen},
  year={2024},
  url={https://www.youtube.com/playlist?list=PLoROMvodv4rMJqxxviPa4AmDClvcbHi6h}
}

@article{sun2019optimization,
  title={Optimization for deep learning: An overview},
  author={Sun, Ruo-Yu},
  journal={Journal of the Operations Research Society of China},
  volume={8},
  number={2},
  pages={249--294},
  year={2020},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s40305-020-00309-6}
}

@article{dvurechensky2021first,
  title={First-order methods for convex optimization},
  author={Dvurechensky, Pavel and Shtern, Shimrit and Staudigl, Mathias},
  journal={EURO Journal on Computational Optimization},
  volume={9},
  pages={100015},
  year={2021},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S2192440621001428}
}

@inproceedings{sun2021list,
  title={Provable Nonconvex Methods/Algorithms},
  author={Sun, Ju},
  year={2021},
  url={https://sunju.org/research/nonconvex/}
}