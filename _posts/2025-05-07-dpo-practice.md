---
layout: distill
title: Challenges of Sample Inefficiency (CSI) - Practical Limitations of Direct Preference Optimization Algorithm
description: In this blog, we compare Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO) from a reinforcement learning perspective. The absence of a critic model, the lack of GAE estimation, and the use of off-policy sampling in DPO result in high variance but unbiased token-wise rewards estimates. This leads to a significant drawback of DPO - sample inefficiency. Due to limited training samples and a reliance on off-policy data, DPO faces the state distribution shift problem. Additionally, as a Bradley-Terry model with limited samples, DPO struggles to distinguish response pairs with substantial token overlap while still attempting to maximize the difference between them. This interplay between the state distribution shift problem and the limitations of the Bradley-Terry model can result in reduced likelihoods for both positive and negative samples.
date: 2025-05-07
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
authors:
  - name: Anonymous


# must be the exact same name as your blogpost
bibliography: 2025-05-07-dpo-practice.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
  - name: Contrasting DPO and PPO from an RL Perspective
    subsections:
    - name: Preliminary
    - name: Multi-Arm Bandit vs. Markov Decision Process
    - name: Monte Carlo Method vs. Temporal Difference Learning
    - name: Bradley-Terry Model vs. Weighted Logistic Model
    - name: REINFORCE vs. Actor-Critic Method
    - name: Off-Policy Method vs. On-Policy Method
  - name: Limitations of DPO
  - name: Conclusion

---

## Introduction

> Reinforcement Learning From Human Feedback (RLHF) has been critical to the success of the latest generation of generative AI models. In response to the complex nature of the classical RLHF pipeline, direct alignment algorithms such as Direct Preference Optimization (DPO) have emerged as an alternative approach. Although DPO solves the same objective as the standard RLHF setup, there is a mismatch between the two approaches <d-cite key="rafailov2024from"></d-cite>.
> 

In this blog, we will explore the primary distinctions between Direct Preference Optimization (DPO) <d-cite key="rafailov2024direct"></d-cite> and Reinforcement Learning from Human Feedback (RLHF), specifically focusing on Proximal Policy Optimization (PPO) <d-cite key="schulman2017proximal"></d-cite> from a reinforcement learning (RL) perspective. Specifically, we highlight the following contrasts:

- DPO conceptualizes the response generation of large language models (LLMs) as a multi-arm bandit problem <d-cite key="bouneffouf2019survey"></d-cite>, while PPO views this process through the lens of Markov Decision Processes (MDP) <d-cite key="knox2012reinforcement"></d-cite>.
- DPO employs a Monte Carlo method <d-cite key="jiang2021reinforcement"></d-cite> to sample pairs of responses generated by LLMs and estimate human preferences through pairwise comparisons. In contrast, PPO uses Generalized Advantage Estimation (GAE), a refined version of n-step Temporal Difference (TD) learning <d-cite key="pong2018temporal"></d-cite>, to estimate the token-wise reshaped rewards of the responses.
- DPO utilizes the Bradley-Terry model <d-cite key="han2020asymptotic"></d-cite> to learn human preferences on responses generated by LLMs from pairwise human preference data. Conversely, PPO uses a weighted logistic model <d-cite key="wilson2015weighted"></d-cite> to learn point-wise rankings of tokens within specific contexts.
- DPO is a variant of the Reinforce Algorithm <d-cite key="sewak2019policy"></d-cite>, also known as the Monte Carlo Policy Gradient Algorithm. On the other hand, PPO is a variant of the Actor-Critic Algorithm <d-cite key="konda1999actor"></d-cite>, which improves upon the Reinforce framework by introducing a separate value function  (the critic) that evaluates the actions generated by the policy (the actor).
- DPO is an off-policy method <d-cite key="degris2012off"></d-cite> since it learns the policy from an offline dataset that might not be generated by the current policy (i.e., the model during DPO training). Conversely, PPO is an on-policy algorithm <d-cite key="deisenroth2013survey"></d-cite>, as it directly relies on the data generated by the current policy for updates.
- DPO does not guarantee an increase in the likelihoods of positive samples in the training.

>DPO, while advantageous in certain aspects, poses challenges in sample efficiency when compared to PPO, making it less practical in data-limited environments.

## Contrasting DPO and PPO from an RL Perspective

In this section, we will introduce the DPO and PPO algorithms, demonstrate their mathematical equivalence, and explore their differences. We aim to provide readers with a comprehensive comparison of DPO and PPO from a traditional RL perspective.

### Preliminary

In a traditional RLHF <d-cite key="ouyang2022training"></d-cite><d-cite key="achiam2023gpt"></d-cite>, the Bradley-Terry model is utilized to represent the preference function as a sigmoid function of the difference between rewards:

$$
p(y\succ y^{'}|x)=\sigma(r(x,y)-r(x,y^{'}))\ \ \ (1)
$$

where $\sigma$ denotes the sigmoid function, which serves as a normalization mechanism. Given an empirical pair-wise human preference dataset $D = (x_i, y \succ y^{'})$, the reward function can be inferred by minimizing the logistic regression loss:

$$
L(r) = -E_{(x,y,y^{'})\sim D}[\log(p(y \succ y^{'}|x))] \ \ \  (2)
$$

$$
L(r) = -E_{(x,y,y^{'})\sim D}[\log(\sigma(r(x, y) - r(x, y^{'})))] \ \ \  (3)
$$

With the learned reward function $r(x, y)$, the objective of RLHF  is to optimize for the policy $\pi$ to maximize the expected reward concurrently minimizing the distance between $\pi$ and some reference policy $\pi_{ref}$ through the following KL-regularized objective function:

$$
J(\pi) = E_{\pi}[r(x,y)] - \beta D_{\text{KL}}(\pi | \pi_{\text{ref}}) \ \ \ (4)
$$

$$
D_{\text{KL}}(\pi | \pi_{\text{ref}}) = E_{\pi}[\log (\frac{\pi}{\pi_{\text{ref}}})] \ \ \ (5)
$$

Finally, we adopt the PPO algorithm or similar approaches to optimize this objective. The integration of a reward model informed by human feedback with PPO has shown notable success in practical applications <d-cite key="ouyang2022training"></d-cite><d-cite key="achiam2023gpt"></d-cite>.

---

Instead of learning a reward function, Rafailov et al. <d-cite key="rafailov2024direct"></d-cite> propose the DPO algorithm to optimize the LLMs directly from human preferences datasets. Specifically, DPO directly optimizes Equation 3 by establishing the relationship between the optimized policy $\pi$ and $r(x, y)$. DPO frames the problem of learning from human preferences as an offline contextual bandit problem, enabling the derivation of a closed-form expression for the optimized policy $\pi$ :

$$
\pi_r(y|x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y)) \ \ \ (6)
$$

where 

$$ Z(x) = \sum_{y}\pi_{ref}(y | x) exp(\frac{1}{\beta}r(x, y)) $$ 

is the *partition function*. See <d-cite key="rafailov2024direct"></d-cite> for a complete derivation. When taking the logarithm of both sides of Equation 6 and then with some algebra, the authors obtain:

$$
r(x,y) = \beta \log\frac{\pi_r(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x) \ \ \ (7)
$$

The authors then apply this re-parameterization to the ground-truth reward $ r^* $ and corresponding optimal model $ \pi^* $ in Equation 3. The loss DPO optimizes:

$$
 \min_\pi E_{(x, y,y^{'})\sim D}[-\log \sigma(\beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \beta \log \frac{\pi(y^{'}|x)}{\pi_{\text{ref}}(y^{'}|x)})]\ \ \ \ (8)
$$

---

Although both DPO and PPO algorithms can achieve the optimal policy $\pi^*$, notable differences exist in their methodologies.

### Multi-Arm Bandit vs. Markov Decision Process

DPO conceptualizes the response generation in LLMs as a multi-arm bandit problem, with the entire response considered a single arm. It treats the user instruction as the state $s$, following the distribution $\rho$, response generation as action $a$, the LLM as the parameterized policy $\pi_{\theta}$; and user preference for the response as $r(s,a)$. Consequently, the objective function of DPO is structured as follows: 

$$
O_d = \max_{\pi_{\theta}}E_{s \sim \rho, a \sim \pi_\theta(s)}(r(s,a))\ \ \ (9)
$$

---

Instead, PPO views the response generation in LLMs from the perspective of the Markov decision process. It considers the user instructions and generated tokens from the LLM as the state $s$, the next predicted token as $a$ , and the combined user instruction and response as $\tau$. The LLM is treated as the parameterized policy $\pi_{\theta}$ and the user preference for the response as $r(\tau)$. Additionally, $s_0$ denotes the initial user instruction,  and $s_i$ represents the combination of user instruction with the first $i$-generated tokens. Consequently, the probability $p_{\pi_{\theta}}(\tau)$ describes the probability of the LLM $\pi_{\theta}$ generating a response $\tau$:

$$
p_{\pi_{\theta}}(\tau) = \rho(s_0)\prod_{i} \pi_{\theta}(a_i|s_i) C(s_{i+1}|s_i, a_{i}) \ \ \ (10)
$$

where 

$$ C(s_{i+1} | s_i, a_{i}) $$ 

represents the state transition matrix. Given the consistent nature of actions and states in the LLM, the state transition matrix can be straightforwardly expressed as:

$$
C(s_{i+1} | s_i, a_{i}) = \mathbf{1}(a_i == s_{i+1}) \ \ \ (11)
$$

where $\mathbf{1}$ denotes an indicator function. Consequently, the objective function of PPO is structured as follows:

$$
 O_p = \max_{\pi_{\theta}}E_{\tau \sim p_{\pi_\theta}}(r(\tau))\ \ \ (12)
$$

where trajectory $\tau$ corresponds to the $(s, a)$ pair in Equation 9.  Consequently, the objective of PPO, denoted as $O_p$ in Equation 12, is identical to that of DPO, denoted as $O_d$ in Equation 9. 

>Although DPO treats response generation as a multi-arm bandit problem, while PPO treats it as an MDP, DPO’s objective is identical to that of PPO.


### Monte Carlo Method vs. Temporal Difference Learning

In the multi-arm bandit problem, rewards for each action (the entire response in the DPO algorithm) are sampled and averaged, akin to a Monte Carlo Method. While DPO does not directly estimate the reward for each response, the probability of generating a response under an instruction can be considered a reshaped reward, as defined in Equation 6. Thus, from a traditional RL perspective, DPO essentially functions as a Monte Carlo Method. 

Specifically, DPO treats the entire response as an action and estimates its reward. This method often overlooks important tokens that directly represent human preferences in response pairs, thereby introducing high variance in reward estimation. In addition, accurately estimating the reward in DPO requires at least $O(m^n)$ samples, where $m$ represents the vocabulary size and $n$ represents the maximum response length. Consequently, DPO is a sample-inefficient method characterized by high variance in reward estimation for actions.

---

Conversely, PPO calculates human preference for the instruction and each prefix of the response, estimating token-wise or dense rewards. This method identifies important tokens that directly reflect human preference in the response pairs. However, it also has the potential to introduce more bias towards these important tokens.

From an RL perspective, PPO employs GAE, an advanced form of n-step TD learning, to estimate the token-wise reshaped rewards. Specifically, TD learning is a method in reinforcement learning that learns the value function of a policy based on the difference between estimated values of states at consecutive steps. TD Error, a crucial element in TD learning, quantifies the difference between the predicted value of a state and the estimated value of the subsequent state, adjusted by the received reward:

$$
\delta_t = r(s_t, a_t) + \gamma V(s_{t+1}) - V(s_t) \ \ \ (13)
$$

GAE combines elements of both TD learning and Monte Carlo approaches to yield a more stable and effective estimate of the advantage function (token-wise reshaped rewards). GAE computes the advantage function as

$$
G_t^{(\gamma, \lambda)} = \sum_{k=t}^T(\gamma \lambda)^{k-t}\delta_k \ \ \ (14)
$$

where $\gamma$ and $\lambda$ are discounting factors that manage the trade-off between bias and variance. 

>Under the Monte Carlo sampling method, which involves random sampling from sufficient samples (at least exponential in size), training on these samples enables both DPO and PPO to accurately estimate the reward of each response. This ensures that both DPO and PPO achieve the optimal policy, $\pi^*$.


>In the absence of sufficient samples, PPO reduces the high variance in reward estimation for each token, providing a more stable and effective reward estimate compared to DPO. While DPO can theoretically be framed as a token-level MDP for credit assignment <d-cite key="rafailov2024from"></d-cite>, it requires more samples than PPO to achieve accurate token-level credit assignment.


### Bradley-Terry Model vs. Weighted Logistic Model

DPO utilizes the Bradley-Terry (BT) model to learn human preferences for responses generated by LLMs. The loss function of BT model is as follows:

$$
L_{\text{BT}} = -\log\delta(y_w - y_l) \ \ \ (15)
$$

However, a significant concern within the classic RLHF framework is whether the BT model can adequately capture complex human preferences <d-cite key="alignmentguidebook"></d-cite><d-cite key="huggingfacearmor"></d-cite>:

- Collapse in generation diversity: A reward-maximizer tends to reduce entropy, leading to less diverse outputs.
- Overlook of minority preferences: The BT framework does not account for personalization and preference conflicts, potentially aligning with the majority group and neglecting the preferences of minority groups.

---

Conversely, PPO employs a weighted logistic model <d-cite key="wilson2015weighted"></d-cite> to learn point-wise rankings of tokens within specific contexts. This method assigns a weighted reward to each token, capturing the detailed human preferences in response pairs. The loss function of a weighted logistic model is as follows:

$$
 L_{\text{WL}} = - \sum_{i} w_i * \text{CrossEntrophy}(\text{logit}_{i}, y_i) \ \ \ (16)
$$

where $w_i$ represents the token-wise reshaped reward of the token $y_i$ within the context $y_{0:i-1}$.

If the reward model uses the BT framework to learn human preferences, both PPO and DPO may experience reduced generation diversity and overlook minority preferences. To mitigate these weaknesses, an ensemble of the reward model and various tools is commonly used in practice.

>DPO easily collapses in generation diversity and overlooks minority preferences, making it difficult to continue improving LLM performance. However, PPO can continue to enhance LLM performance when supported by a robust reward model in practice.


### REINFORCE vs. Actor-Critic Method

 DPO can be seen as a variant of the REINFORCE <d-cite key="sewak2019policy"></d-cite> algorithm, also known as the Monte Carlo Policy Gradient Algorithm. Specifically, DPO is theoretically equivalent to the following process:

- First, it employs the Bradley-Terry (BT) framework to learn a reward model from the human preference dataset.
- Then, it employs the REINFORCE algorithm to optimize the LLM policy based on this reward model. During training, the REINFORCE algorithm samples queries from the original human preference dataset and minimizes the loss on these samples.

Due to its reliance on Monte Carlo estimation, REINFORCE exhibits high variance and, despite its simplicity, can suffer from slow convergence.

---

To address these weaknesses, Sutton et al. <d-cite key="konda1999actor"></d-cite> proposed the Actor-Critic algorithm. By combining the policy gradient method (actor) with a value function estimate (critic), the Actor-Critic reduces variance in policy updates, leading to more stable and faster learning. PPO is a variant of the Actor-Critic algorithm that also employs a value function to estimate the token-wise reshaped rewards. 

>Compared to DPO, PPO reduces variance in policy updates, resulting in more stable and faster learning by employing a value function to estimate token-wise reshaped rewards.


### Off-Policy Method vs. On-Policy Method

In the DPO algorithm, human preference data does not need to be collected from the current policy, making DPO an off-policy algorithm. Specifically, the action space, particularly the distribution of sampled responses from the trained policy, differs significantly from the sample distribution of the offline dataset. As a result, the current policy is trained on samples it rarely generates, leading to a state distribution shift problem. Furthermore, even if the responses from the offline dataset are sampled from the SFT model, the distribution of generated responses gradually shifts during training, leading to a state distribution shift problem.

---

PPO is an on-policy, online method that begins by sampling responses from the current policy. These responses are evaluated by a reward model, and the generated responses, along with their rewards, are used to further train the current policy. While the reward model also encounters OOD issues, the impact is not as severe.

>DPO is an off-policy method that learns from actions taken by various policies, offering potential flexibility and efficiency in learning. However, off-policy samples often cause state distribution shift issues, leading to model collapse. In contrast, PPO uses on-policy samples for model updates, resulting in a more time-consuming but stable training process.


## Limitations of DPO

From a reinforcement learning perspective, DPO is a variant of the Monte Carlo Policy Gradient Algorithm. The absence of a critic model, the lack of GAE estimation, and the use of off-policy sampling result in high variance but unbiased token-wise reward estimates in DPO, leading to its principal shortcoming: sample inefficiency.

---

With insufficient samples in an offline dataset, DPO is trained predominantly on samples it rarely generates (off-policy samples), leading to a state distribution shift problem. Specifically, when trained with off-policy samples, LLM learns only specific tokens in the positive responses while neglecting others. These tokens are typically those easily generated by the original policy. This results in the policy easily generating responses with these tokens instead of the positive response. For example, an LLM might easily generate sequences like ABD and ACE. If the positive response is ABE and the negative response is ADE, after DPO training, the LLM learns the AB pattern but neglects the E token, which is rarely generated in the context of AB. Consequently, it easily generates ABD instead of ABE. Worse, the model easily learns to reject the response ADE, which might generalized to the pattern A~E, thereby reducing the probability of generating ABE.

---

As a Bradley-Terry model, DPO maximizes the differences between positive and negative samples. However, it does not guarantee an increase in the likelihoods of positive samples. For instance, if the initial likelihood of a positive sample is $0.7$ and that of a negative sample is $0.3$, the difference between their log probabilities is $\log⁡ 0.7− \log ⁡0.3$, which equals $0.368$. If the likelihood of the positive sample changes to $10^{−1}$ and that of the negative sample changes to $10^{−7}$, the difference between their log probabilities becomes $6$. Despite the decrease in the positive sample's likelihood, this change still maximizes the difference between the positive and negative samples. When there is substantial token overlap between the positive and negative samples, DPO struggles to distinguish the significant tokens in the responses, often simultaneously increasing or decreasing the likelihoods of both sample types. To minimize the DPO loss, the model tends to decrease the likelihoods of both positive and negative samples. 

Furthermore, BT model tends to overfit simpler pairwise samples and neglect more challenging ones with limited samples. The overfitting of simpler pairwise samples can lead to model collapse, reducing the probability of generating other chosen responses.

---

The interplay between the state distribution shift problem and the inherent limitations of the Bradley-Terry model may result in reduced likelihoods for both positive and negative samples <d-cite key="rafailov2024from"></d-cite><d-cite key="pal2024smaug"></d-cite>. 

## Conclusion

We compared PPO and DPO from multiple perspectives and highlighted DPO's limitations, and these results indicate that DPO tends to require a significantly larger volume of data than PPO to achieve similar convergence stability, making PPO the more efficient choice in practice.
