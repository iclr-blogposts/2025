---
layout: distill
title: Distilling LLMs into MiniLLMs
description: "In this blog post, we propose to investigate a technique to distill LLMs into smaller models. Relying on the paper <em>MiniLLM: Knowledge Distillation of Large Language Models</em>, published in March 2024 by Yuxian Gu et al., we summarize and discuss its key findings. We supplement their work by reproducing their results and explore its generalizability."
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Param Damle
    url: "https://app.paramdamle.com/about"
    affiliations:
      name: Carngie Mellon University
  - name: Clement Ou
    url: "https://clementou.com/"
    affiliations:
      name: Carngie Mellon University
  - name: Ines Vignal
    url: "https://www.linkedin.com/in/ines-vignal/"
    affiliations:
      name: Carngie Mellon University

# must be the exact same name as your blogpost
bibliography: 2025-04-28-distill-example.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
    subsections:
    - name: Subsection

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## MiniLLM: Knowledge Distillation of Large Language Models

LLMs have massive computational demand
Knowledge Distillation (KD) reduces compupational demand. train small student with supervision from large teacher.
Types https://link.springer.com/article/10.1007/s11263-021-01453-z
  black box KD only teacher generated texts available -> shown promising results with prompt-response pairs from big LLM APIs
  white box teacher model's ouput distribution or intermediate hidden states available -> better when white box models are available, better signals learned from output distribution
    mostly studied for small  <1B parameters language understanding/classification models (language _understanding_ with BERT https://arxiv.org/pdf/1910.01108 or confusingly named miniLLM language understanding paper https://arxiv.org/pdf/2002.10957)
    argue standard KD objectives subpar for generation

distill knowledge from larger model. generate and generalize. don't overvalue low probability regions. maintain truthfulness (relative to parent distribution). disincentivize brevity.

forward KL : KL[p|q] forces q to cover all modes of p
classification this works well due to finite many modes to cover
less finite for open ended generation, model capacity of q may not be able to cover all modes of p
According to https://arxiv.org/pdf/1905.13472, forward distribution is zero avoiding, so model will spread itself out over every mode, reducing precision and boosting low probabiliity regions rather than boosting an "uncertain" signal due to seeking to minimize divergence in arithmetic expectation TODO back up or explain in simple terms
Describe KLD in math.
Reverse KLD, or KL q theta || p 
avoid learning long tail low probability outcomes, focus on getting the frequent ones right, critical to truthfulness and reliability TODO cite hallucination issues
zero forcing

Critique: what if we want to capture some rare modes and not produce sheeple? https://openreview.net/pdf?id=RovX-uQ1Hua
1. multiple distinct responses -> one correct response is sufficient (tradeoff in favor of correctness)
2. linguistically complex responses -> yes, lower distinct 4 gram scores and loss, but not by much I guess?
3. generated content covers real data distribution

sequence level: teacher sends sample from p, student sends q, KL p||q. MinILLM: teacher sends p, student samples y from q, KL q || p

rather than try to increase probability for each sample sent by teacher, try to send samples that resonate with the teacher

Policy Gradient
increase conditional p to maximize alignment with parent but keep q low to allow probability space to be diversified by other options
Policy gradient suffers from reward hacking, TODO https://openreview.net/pdf?id=yb3HOXO3lX2
Making loss gradient go to 0 means Rt go to 0, which is most favorable with no tokens at all!

1. single step decomposition to reduce variance: single step generation quality of token at t' is critical to training variance, error in front tokens accumulates to latter tokens. extracts a term that is computable relative to parameters instead of doing monte carlo sampling (TODO summing over vocabulary expensive?) apparently reduces variance and accelerates convergence
This means less spikes in reverse KLD every step
2. teacher mixed smapling to alleviate reward hacking: e.g. repeated or degenerate phrases. mix teacher and student distribution weighted avg with alpha = 0.2. more alpha is more teacher mix in. suppresses low quality generation because the loss is sampled with a combo of parent and student distributions, and evaluated on the student's share (multiplied with a different factor to avoid variances accumulating)
RKLD decreases more without this, but generated sequences are short, meaningless repeated strings (Figure 8)
3. length normalization to eliminate length bias: long sequences have low R, or divergence from here onward? (intuition, p will have lower prob than q, so multiplying a bunch of fractions yields a fraction, log of which is very negative) which produces very large positive delta loss, and model wants to move to 0. basically make it the log geometric mean, or arithmetic mean of log

KD previously applied to white box or small model to imitate black box mirroring ChatGPT
distill knowledge of lang gen white box models
exigence: open source LLMs on the rise? TODO cite
replace common forward KLD with reverse KLD, more suitable for lang gen to prevent student from overestimating low probability regions

train the student model on large long document corpus to get sensibility. fine tune model on a text generation task dataset D. compute loss of single and long with clipping for stability? TODO https://arxiv.org/pdf/1707.06347

add language modeling loss to penalize repeating its foundational learning

Methods: various models, 120M -> 13B size, evaluated on 5 datasets (list them?) split by response length

compare with big models. OpenWeb for GPT2 and RoBERTa training for other models

Eval:
RougeL: precision for alrge scale instruction following TODO https://aclanthology.org/W04-1013.pdf
human judgement: ask humans to pick a winner between 2 outputs
GPT4 evaluation: ask GPT to compare

SFT: fine tune directly on D supervised by TODO golden responses?
KD: fine tune on D using teacher distribution on token by token
SeqKD: student model fine tuned on data generated by teacher ? TODO specify

Results: good generality
most precise alignment with ground truth, better than teacher
Teacher forcing, training inference discrepancy, exposure bias
MiniLLM improves consistently regardless of scale
better human preference

student model may not always proportionally scale with teacher size due to what? TODO https://arxiv.org/pdf/1902.03393

Exposure bias due to difference between teacher forcing training and free run generation TODO 
avoided since student sees its own responses in training

Models trained with policy optimization are poorly calibrated TODO https://cdn.openai.com/papers/gpt-4.pdf
Test on classification tasks, with zero shot classification instructions

Differences in length show that MiniLLM outperforms regardless of golden length range, with no clear pattern
poor performance in ground truth < 5 tokens due to lack of representation in data
intuition: more tokens = bigger space = more nodes = better fit for reverse KL


metrics:
1. higher overall quality
2. lower exposure bias
3. better calibration
4. higher long text generation performance

Method scalable from 120M to 13B parameters