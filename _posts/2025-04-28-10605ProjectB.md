---
layout: distill
title: Distilling LLMs into MiniLLMs
description: "In this blog post, we propose to investigate a technique to distill LLMs into smaller models. Relying on the paper <em>MiniLLM: Knowledge Distillation of Large Language Models</em>, published in March 2024 by Yuxian Gu et al., we summarize and discuss its key findings. We supplement their work by reproducing their results and explore its generalizability."
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Param Damle
    url: "https://app.paramdamle.com/about"
    affiliations:
      name: Carngie Mellon University
  - name: Clement Ou
    url: "https://clementou.com/"
    affiliations:
      name: Carngie Mellon University
  - name: Ines Vignal
    url: "https://www.linkedin.com/in/ines-vignal/"
    affiliations:
      name: Carngie Mellon University

# must be the exact same name as your blogpost
bibliography: 2025-04-28-distill-example.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
    subsections:
    - name: Subsection

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Introduction

This is an introduction


### Subsection

This is a subsection