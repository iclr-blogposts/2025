---
layout: distill
title: Can Large Language Models do Program Analysis for Directed Fuzzing?
description:  
An empirical study comparing the performance of Large Language Models against traditional directed fuzzers on program analysis tasks, 
with a focus on path discovery and target reachability using the Fuzzle benchmark framework.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Haochen Zeng
    # url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    affiliations:
      name: University of California, Riverside
  - name: Mingjun Yin
    # url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
    affiliations:
      name: University of California, Riverside
  - name: Chengyu Song
    url: "https://www.cs.ucr.edu/~csong/"
    affiliations:
      name: University of California, Riverside

# must be the exact same name as your blogpost
bibliography: 2025-04-28-can-llm-generate-vulnerability-exploits.bib

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
  - name: Large Language Models in Program Analysis
    # subsections:
    # - name: Interactive Figures
  - name: Fuzzle Benchmark Framework
  - name: Experimental Evaluation
      subsections:
      - name: LLM Model Comparison
      - name: Comparison with Directed Fuzzers
  - name: Discussion
  - name: Conclusion

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Introduction
Directed fuzzing has emerged as a critical technique in software testing and security analysis. Unlike traditional coverage-guided fuzzing that aims to explore as much code as possible, directed fuzzing focuses on reaching specific program locations efficiently. This capability is particularly valuable for tasks like bug reproduction, patch testing, and verifying potential vulnerabilities identified by static analysis tools.
Traditional directed fuzzers like AFLGo use distance metrics based on static control flow graphs (CFG) to guide the fuzzing process toward target locations. However, these approaches face several challenges:

Static CFG-based distance calculations can be imprecise
Control-flow analysis often misses critical data-flow dependencies
Fuzzers can waste significant time exploring paths that cannot reach the target

## Large Language Models in Program Analysis
With the rise of Large Language Models (LLMs), we're seeing new possibilities for program analysis tasks. LLMs have demonstrated impressive capabilities in understanding code structure, identifying patterns, and even generating test cases. This raises an interesting question: Could LLMs provide more intelligent guidance for directed fuzzing compared to traditional static analysis approaches?
LLMs in Program Analysis
Large Language Models have shown promising results in various program analysis tasks:

Code Understanding: LLMs can comprehend program structure, control flow, and data dependencies through their training on vast code repositories.
Pattern Recognition: They can identify common programming patterns and potential vulnerabilities by leveraging their knowledge of similar code structures.
Semantic Analysis: Unlike traditional static analyzers that work with syntactic patterns, LLMs can understand semantic relationships and program behavior at a higher level.

For directed fuzzing specifically, LLMs could potentially:

Predict more accurate path feasibility
Identify relevant data dependencies
Suggest promising input mutations
Recognize patterns that indicate dead ends or promising directions

## The Fuzzle Benchmark
To evaluate LLM performance on directed fuzzing tasks, we'll use the Fuzzle benchmark framework. Fuzzle is particularly well-suited for this evaluation because:

Controlled Environment: It generates synthetic programs where bug locations are known and path constraints are well-defined.
Visualization: It represents programs as mazes, making it easier to visualize and understand the exploration process.
Real-world Patterns: Despite being synthetic, Fuzzle incorporates path constraints from real CVEs, ensuring relevance to practical scenarios.
Scalable Complexity: We can generate benchmarks of varying sizes to test how different approaches scale.

## Experimental Setup and Results

### LLM Model Comparison
We evaluated three leading LLM models on directed fuzzing tasks:

GPT-4
Claude-3
LLaMA-2

For each model, we provided:

The program's control flow graph
Target location information
Current execution state
Available mutations

Evaluation Metrics:

Time to reach target (TTE)
Number of paths explored
Accuracy of path feasibility predictions
Quality of suggested mutations

[Include comparison table and analysis here]

### Comparison with Traditional Directed Fuzzers
We compared the best-performing LLM approach against state-of-the-art directed fuzzers:

AFLGo
Beacon
SelectFuzz
DAFL
MazeRunner

Key Findings:

Path Prediction Accuracy:

LLMs showed superior accuracy in predicting path feasibility
However, they required significantly more computation time


Exploration Efficiency:

Traditional fuzzers performed more explorations but with lower precision
LLM-guided approaches required fewer explorations but had higher overhead


Scalability Challenges:

LLM performance degraded with larger programs due to context window limitations
Traditional fuzzers maintained more consistent performance across program sizes



[Include detailed performance graphs and analysis]

## Discussion
Our experiments revealed several interesting insights about using LLMs for directed fuzzing:
Advantages:

Semantic Understanding: LLMs demonstrated superior ability to understand program semantics and identify relevant paths.
Pattern Recognition: They effectively recognized patterns from similar code structures to guide exploration.
Adaptive Learning: LLMs could adjust their strategies based on previous exploration results.

Challenges:

Computational Overhead: The time required for LLM inference is significantly higher than traditional static analysis.
Context Window Limitations: Large programs exceed current LLM context windows, requiring complex chunking strategies.
Consistency: LLM outputs can be non-deterministic, potentially leading to inconsistent guidance.

Future Directions:

Hybrid Approaches: Combining LLM insights with traditional fuzzing techniques could offer better performance.
Specialized Models: Training smaller, specialized models for program analysis could reduce overhead.
Improved Prompting: Developing better prompting strategies could enhance LLM performance on program analysis tasks.

## Conclusion
While LLMs show promising capabilities for program analysis and directed fuzzing, they currently face significant practical limitations. Their semantic understanding and pattern recognition abilities are impressive, but the computational overhead and scalability challenges make them impractical for many real-world applications.
However, the insights gained from this study suggest several promising directions for future research:

Developing specialized models for program analysis
Creating hybrid approaches that combine LLM insights with traditional techniques
Improving prompt engineering for program analysis tasks

As LLM technology continues to advance, we expect to see more practical applications in program analysis and directed fuzzing, particularly in scenarios where deeper semantic understanding is crucial for effectiveness.