---
layout: distill
title: Can Large Language Models do Program Analysis for Directed Fuzzing?
description:  
An empirical study comparing the performance of Large Language Models against traditional directed fuzzers on program analysis tasks, 
with a focus on path discovery and target reachability using the Fuzzle benchmark framework.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Haochen Zeng
    # url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    affiliations:
      name: University of California, Riverside
  - name: Mingjun Yin
    # url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
    affiliations:
      name: University of California, Riverside
  - name: Chengyu Song
    url: "https://www.cs.ucr.edu/~csong/"
    affiliations:
      name: University of California, Riverside

# must be the exact same name as your blogpost
bibliography: 2025-04-28-can-llm-generate-vulnerability-exploits.bib

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
  - name: Large Language Models in Program Analysis
    # subsections:
    # - name: Interactive Figures
  - name: Fuzzle Benchmark Framework
  - name: Experimental Evaluation
      subsections:
      - name: LLM Model Comparison
      - name: Comparison with Directed Fuzzers
  - name: Discussion
  - name: Conclusion

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Introduction
The rise of Large Language Models (LLMs) has revolutionized many areas including program analysis, demonstrating impressive capabilities in understanding code structure, semantics, and identifying patterns. 
Their ability to comprehend and analyze complex program behaviors opens new possibilities for software testing and security analysis.
Directed fuzzing is a testing technique that automatically generates inputs attempting to reach and trigger buggy program state at specific target locations.

This leads to a critical research question: 
Can LLMs assist or improve upon traditional directed fuzzing approaches in efficiently reaching target program locations?

This blog explores this question by evaluating the performance of various LLM models and comparing LLMs with traditional fuzzers using a maze program synthesized from the Fuzzle benchmark.
Fuzzle synthesizes the bug method into a maze-generation process, where triggering a bug is analogous to finding the maze's exit.
Successfully solving the maze demonstrates a model's ability to generate correct inputs that reach the buggy function.


Our results shows:
- The latest model, GPTo1, can solve the maze independently, without human intervention.
- With the Chain-of-Thought reasoning generated by GPTo1, even the smaller model, GPTo1-mini, successfully solves the maze.
- When provided with step-by-step subtasks, GPT4o and Claude 3.5 Sonnet can also navigate and solve the maze effectively.


## Directed Fuzzing

Software testing and bug finding have always been critical challenges in computer security. 
Among various testing techniques, fuzzing has emerged as one of the most effective approaches for finding security vulnerabilities. 
At its core, fuzzing consists of two key components:

1. An input generator that produces and mutates test inputs at high speed
2. A runtime monitor (like AddressSanitizer) that detects security violations during execution
<!-- Traditional coverage-guided fuzzers like AFL (American Fuzzy Lop) aim to explore as much code as possible, based on the assumption that less-tested code is more likely to contain bugs. 
AFL achieves this by:
- Instrumenting the program to track code coverage
- Using genetic algorithms to mutate inputs that trigger new paths
- Maintaining a queue of interesting inputs for further mutation -->

In many practical scenarios, developers need to focus on testing specific program locations. 
For example:
- Verifying if a newly patched vulnerability is fixed
- Testing recently modified code
- Checking potential bugs reported by static analyzers

This is where directed fuzzing comes in. 
To understand the challenges, let's look at a simplified example that mimics a vulnerable program:

{% highlight c++ linenos %}

#include <stdio.h>
#include <stdlib.h>

void cell_A(int choice) {
    printf("Input value: %d\n", choice);
    
    // Path 1: value < 0
    if (choice < 0) {
        cell_B(choice);         // Dead end
    }
    // Path 2: 0 <= value <= 50 
    else if (choice <= 50) {
        cell_A(choice);         // Infinite Loops back
    }
    // Path 3: value > 50
    else {
        cell_C(choice);         // Can reach target
    }
}

void cell_B(int choice) {
    if (choice == 1) { // the condition is always false
      bug();           // can never reach target bug()
    }
    // Dead end
    printf("Reached dead end\n");
}

void cell_C(int choice) {
    // Complex condition determining path
    if (choice % 7 == 0) {
        cell_D(choice);    // Target: potential vulnerability
    } else {
        cell_B(choice);    // Return to dead end
    }
}

void cell_D(int choice) {
    bug();
}

void bug(){
    printf("Found vulnerability!\n");
    abort();  // Vulnerability triggered
}

int main() {
    int input;
    scanf("%d", &input);    // Read user input
    cell_A(input);          // Start exploration
    return 0;
}
{% endhighlight %}

### Understanding the Example
This code simulates a program with a potential vulnerability in `cell_D` that we want to trigger. To reach it:
1. The program takes a single integer input.
2. Starting from `cell_A`, it navigates through different paths based on input conditions.
3. Only inputs that are both greater than 50 AND divisible by 7 can reach the target `cell_D`.
4. All other paths lead to `cell_B`, which is a dead end, or lead to 'cell_A' again, which is an infinite recursion.

The purpose of this example is to demonstrate why directed fuzzing is challenging. Even in this simple program:

### Challenge 1: Imprecise Distance Metrics Computed from Static Analysis
Traditional directed fuzzers like AFLGo use static control flow graphs (CFG) to calculate distance to the target. 
In our example:
```
cell_A -> cell_C -> cell_D -> bug  (Distance = 3 steps)
cell_A -> cell_B -> bug            (Distance = 2 step)
```
The CFG might suggest `cell_B` is "closer" to the target, but it's actually a dead end. 
This shows how static distance metrics can be misleading.
<!-- Even worse, control-flow analysis overlooks critical data dependencies, which are essential for understanding the true execution paths that influence whether the target can be reached or not. -->

### Challenge 2: Inefficient Excution Path Exploration during Dynamic Analysis

To reach `cell_D` (our target), a test input must satisfy multiple conditions:
- Must be greater than 50 (condition in `cell_A`)
- Must be divisible by 7 (condition in `cell_C`)

Traditional fuzzers face two major inefficiencies here:

1. **Random Exploration**: 
A fuzzer might waste significant time generating inputs that don't satisfy these conditions at all.

2. **Lack of Fine-grained Control**: 
Even when a fuzzer successfully generates an input that reaches `cell_C` (a step closer to the target), its next mutation is still random. This means it might generate a new input that falls back to `cell_B` (a dead end), losing the progress it just made. 
This "two steps forward, one step back" behavior makes the exploration process highly inefficient.

Think of it like trying to solve a maze while blindfolded - 
even if you're told you're getting closer to the exit, your next step is still a guess that might take you backwards.

## Understanding the Testbed

To systematically evaluate how LLMs perform on directed fuzzing tasks, we need a controlled testing environment. 
This is where the Fuzzle benchmark comes in. 
Fuzzle is a framework that generates synthetic programs that mimic real-world directed fuzzing challenges in a controlled way.

Let's look at a simple 10x10 maze program generated by Fuzzle. 
Think of it as a more complex version of our previous room example, but instead of 4 rooms (A, B, C, D), it has 100 functions (func_0 through func_99). 
Here's what makes it interesting:

1. **Program Structure**
   - Each function represents a room in the maze
   - Function calls represent doors between rooms
   - The program starts at func_0 (entrance)
   - func_bug represents our target (exit)

2. **Navigation Rules**
   - Each function reads one byte from the input
   - Based on this byte's value, it decides which function to call next
   - For example: if input < 0 go left, if input >= 0 go right
   - Wrong decisions lead to dead ends or infinite loops

The program structure can be visualized as a maze:

{% include figure.html path="assets/img/2025-04-28-can-llm-generate-vulnerability-exploits/maze_viz.png" class="img-fluid" %}

- White cells: Functions in the program.
- Black cells: No connection exists between functions.
- Yellow cell: Critical path functions that must be traversed to reach the target.
<!-- - Red cell: 
- Green lines: -->
- Entry point (top-left): Main function where program execution begins.
- Target (bottom-right): Bug function we aim to reach.

To find the bug, we must generate program inputs that follow the yellow path exactly. Taking any wrong turn leads to:

- A dead end.
- A path that loops back.
- A path that wanders away from the target.

This visualization helps us understand why directed fuzzing is challenging:
1. The maze is large (100 rooms vs our 4-room example)
2. Many paths lead to dead ends
3. The correct path requires satisfying multiple conditions
4. Some paths may loop back to previously visited functions

### LLM Model Comparison
We evaluated three leading LLM models on directed fuzzing tasks:

GPT-4
Claude-3
LLaMA-2

For each model, we provided:

The program's control flow graph
Target location information
Current execution state
Available mutations

Evaluation Metrics:

Time to reach target (TTE)
Number of paths explored
Accuracy of path feasibility predictions
Quality of suggested mutations

[Include comparison table and analysis here]

### Comparison with Traditional Directed Fuzzers
We compared the best-performing LLM approach against state-of-the-art directed fuzzers:

AFLGo
Beacon
SelectFuzz
DAFL
MazeRunner

Key Findings:

Path Prediction Accuracy:

LLMs showed superior accuracy in predicting path feasibility
However, they required significantly more computation time


Exploration Efficiency:

Traditional fuzzers performed more explorations but with lower precision
LLM-guided approaches required fewer explorations but had higher overhead


Scalability Challenges:

LLM performance degraded with larger programs due to context window limitations
Traditional fuzzers maintained more consistent performance across program sizes



[Include detailed performance graphs and analysis]

## Discussion
Our experiments revealed several interesting insights about using LLMs for directed fuzzing:
Advantages:

Semantic Understanding: LLMs demonstrated superior ability to understand program semantics and identify relevant paths.
Pattern Recognition: They effectively recognized patterns from similar code structures to guide exploration.
Adaptive Learning: LLMs could adjust their strategies based on previous exploration results.

Challenges:

Computational Overhead: The time required for LLM inference is significantly higher than traditional static analysis.
Context Window Limitations: Large programs exceed current LLM context windows, requiring complex chunking strategies.
Consistency: LLM outputs can be non-deterministic, potentially leading to inconsistent guidance.

Future Directions:

Hybrid Approaches: Combining LLM insights with traditional fuzzing techniques could offer better performance.
Specialized Models: Training smaller, specialized models for program analysis could reduce overhead.
Improved Prompting: Developing better prompting strategies could enhance LLM performance on program analysis tasks.

## Conclusion
While LLMs show promising capabilities for program analysis and directed fuzzing, they currently face significant practical limitations. Their semantic understanding and pattern recognition abilities are impressive, but the computational overhead and scalability challenges make them impractical for many real-world applications.
However, the insights gained from this study suggest several promising directions for future research:

Developing specialized models for program analysis
Creating hybrid approaches that combine LLM insights with traditional techniques
Improving prompt engineering for program analysis tasks

As LLM technology continues to advance, we expect to see more practical applications in program analysis and directed fuzzing, particularly in scenarios where deeper semantic understanding is crucial for effectiveness.