---
layout: distill
title: "Bridging the Modality Gap: Enhancing Document Retrieval with Multimodal Embeddings"
description: In an era where documents blend rich textual and visual content, traditional retrieval systems fall short. This post explores how Multimodal Embeddings bridges the modality gap, introducing advanced models like VISTA and ColPali, innovative evaluation methods, and the considerations in deploying such systems.
date: 2024-11-10
authors:
  - name: Nikhil Reddy
    affiliations:
      name: Mila, Quebec
bibliography: 2025-04-28-MVS.bib
toc:
  - name: Introduction
  - name: Challenges in Document Retrieval
    subsections:
    - name: Modality-Specific Challenges
    - name: The Modality Gap
  - name: Limitations of Traditional Models
    subsections:
    - name: Text-Based Models
    - name: Vision-Based Models
  - name: Advanced Evaluation Metrics for Ranking
    subsections:
    - name: Mean Reciprocal Rank (MRR)
    - name: Normalized Discounted Cumulative Gain (nDCG)
    - name: Mean Average Precision (MAP)
  - name: Recent Advancements in Multimodal Retrieval Models
    subsections:
    - name: "VISTA: Vision-Augmented Text Embeddings"
    - name: "ColPali: Bridging the Modality Gap with Vision-Language Models"
  - name: Interpretability of ColPali in Financial Documents
    subsections:
    - name: Importance of Interpretability
    - name: Case Studies
  - name: Innovative Evaluation Methods
    subsections:
    - name: Feature Search Experiments
    - name: Clustering-Based Benchmarking Approach
  - name: Experimental Evaluations
    subsections:
    - name: Dataset Description
    - name: Performance Comparison
  - name: Ethical Considerations
  - name: Conclusion
  - name: Future Directions
---

In today's data-driven world, the volume and complexity of information have grown exponentially. Documents are no longer confined to plain text; they now encompass a rich blend of images, charts, tables, and intricate layouts. This evolution presents a significant challenge: **How do we effectively retrieve and analyze information from such complex, multimodal documents?** \\
Traditional retrieval systems, primarily designed for text-only data, often falter when faced with this complexity. They struggle to extract and interpret the valuable information embedded within various modalities. This limitation hampers our ability to harness the full potential of the data at our disposal. \\
Enter **Multimodal Embeddings**—an innovative approach that leverages both textual and visual data to revolutionize document retrieval. By bridging the **modality gap** between different data types, MVS promises to make information retrieval more accurate and efficient than ever before. 

In this blog post, we'll delve into: 
- The unique challenges that modern, complex documents pose for retrieval tasks.
- The limitations of traditional text-only and vision-only models.
- Understanding the **modality gap** and its impact on multimodal retrieval.
- Exploring cutting-edge multimodal models like **VISTA** <d-cite key="Zhou2024VISTA"></d-cite> and **ColPali** <d-cite key="Faysse2024ColPali"></d-cite>.
- Investigating the interpretability of these models in real-world applications.
- Incorporating empirical results to illustrate the performance of these models.
- Novel benchmarking strategies designed to evaluate these advanced models effectively.
- Ethical considerations in deploying multimodal retrieval systems.

## Introduction

<div class="row mt-3">
{% include figure.html path="assets/img/2025-04-28-MVS/DSE.png" class="img-fluid" %}
</div>
<div class="caption">
    Figure 1: Integration of text and visual modalities in document retrieval.
</div>

The advent of sophisticated document formats that integrate text, images, and complex layouts has rendered traditional text-based retrieval systems inadequate. The richness of multimodal documents requires systems that can understand and process multiple data types simultaneously. The **modality gap**—the disconnect between different types of data representations—poses a significant hurdle in achieving effective retrieval<d-cite key="Liang2022MindGap"></d-cite>.

## Challenges in Document Retrieval

<div class="row mt-3">
{% include figure.html path="assets/img/2025-04-28-MVS/main.png" class="img-fluid" %}
</div>
<div class="caption">
    Figure 2: Illustration of the increasing levels of detail in images, from basic visuals to more information-dense representations.
</div>

### Modality-Specific Challenges

Retrieving information from today's complex documents is a multifaceted problem that requires a nuanced approach. Documents now often contain:

- **Text**: Dense paragraphs, bullet points, and annotations.
- **Images**: Photographs, diagrams, and illustrations.
- **Tables and Charts**: Structured data representations.
- **Complex Layouts**: Multi-column formats, sidebars, and embedded multimedia.

Each modality presents unique technical obstacles:

1. **Textual Data**:
   - **Language Ambiguity**: Synonyms, homonyms, and context-dependent meanings.
   - **Multilingual Content**: Documents may contain multiple languages or dialects.

2. **Visual Data**:
   - **Image Quality**: Low-resolution images can hinder recognition.
   - **Complex Visuals**: Diagrams and charts may contain dense information that's hard to parse.

3. **Structural Layout**:
   - **Non-linear Reading Paths**: Multi-column texts and inserts can confuse linear text processors.
   - **Embedded Elements**: Images and tables interwoven with text complicate parsing.

### The Modality Gap

<div class="row ml-5">
{% include figure.html path="assets/img/2025-04-28-MVS/modality_gap.png" class="img-fluid"%}
</div>
<div class="caption">
    Figure 2: Visualization of the modality gap between text and image embeddings.
</div>

The **modality gap** refers to the disconnect between the feature representations of different data types. Text and images are encoded differently, making it difficult for models to relate information across modalities.

**Implications:**

- **Ineffective Retrieval**: Models may retrieve documents that match in one modality but are irrelevant in another.
- **Increased Complexity**: Bridging the gap requires sophisticated architectures that can handle multiple data types simultaneously.
- **Higher Computational Demands**: Multimodal models are often larger and require more computational power.

## Limitations of Traditional Models

### Text-Based Models

Text-based retrieval models—leveraging techniques like TF-IDF, BM25, and transformer-based embeddings such as BERT—have been the cornerstone of information retrieval. They excel at understanding and retrieving information when text is the primary medium.

**Limitations:**

- **Blind to Visual Content**: Unable to interpret images, charts, or layouts.
- **Ignoring Spatial Relationships**: Can't understand the importance of where text appears in a document.
- **Struggling with Non-Linear Layouts**: Fail to process documents with complex formatting.

### Vision-Based Models

Vision-based retrieval models employ convolutional neural networks (CNNs) or vision transformers to process and extract features from the visual content of documents.

**Limitations:**

- **Text Interpretation**: Poor at reading and understanding text within images.
- **Fine-Grained Details**: May miss small fonts or intricate diagrams.
- **Semantic Gap**: Lack understanding of the textual semantics associated with visual elements.

## Advanced Evaluation Metrics for Ranking

Evaluating retrieval systems, especially those handling multimodal data, demands metrics that account for both relevance and ranking position.

### Mean Reciprocal Rank (MRR)

**Definition:**

The MRR measures how quickly a system retrieves the first relevant document.

$$
\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}
$$

Where:
- \( |Q| \) is the number of queries.
- \( \text{rank}_i \) is the rank position of the first relevant document for the \( i \)-th query.

**Importance:**

- **User Satisfaction**: Higher MRR indicates users find relevant documents sooner.
- **System Efficiency**: Reflects the system's ability to prioritize relevant results.

### Normalized Discounted Cumulative Gain (nDCG)

**Definition:**

nDCG evaluates ranking quality by considering the position of relevant documents and assigning higher importance to top-ranked results.

$$
\text{nDCG}_p = \frac{1}{\text{IDCG}_p} \sum_{i=1}^p \frac{2^{\text{rel}_i} - 1}{\log_2(i+1)}
$$

Where:

- \( p \) is the number of results considered.
- \( \text{rel}_i \) is the relevance score of the result at position \( i \).
- \( \text{IDCG}_p \) is the ideal DCG up to position \( p \).

**Importance:**

- **Relevance and Rank**: Balances both factors to provide a holistic evaluation.
- **Comparison Across Queries**: Normalization allows for fair comparison between different queries.

### Mean Average Precision (MAP)

**Definition:**

MAP computes the mean of average precision scores across all queries.

$$
\text{MAP} = \frac{1}{|Q|} \sum_{q \in Q} \frac{1}{n_q} \sum_{k=1}^{n_q} \text{Precision}(k) \times \text{rel}(k)
$$

Where:

- \( n_q \) is the number of retrieved documents for query \( q \).
- \( \text{Precision}(k) \) is the precision at cutoff \( k \).
- \( \text{rel}(k) \) is a binary indicator of relevance at position \( k \).

**Importance:**

- **Comprehensive Evaluation**: Accounts for all relevant documents, not just the first.
- **Balanced Metric**: Reflects both precision and recall across the ranking.

## Recent Advancements in Multimodal Retrieval Models

### VISTA: Vision-Augmented Text Embeddings

<div class="row mt-3">
{% include figure.html path="assets/img/2025-04-28-MVS/vista.png" class="img-fluid"%}
</div>
<div class="caption">
    Figure 3: VISTA's architecture integrates visual tokens into text embeddings.
</div>

**Overview:**

VISTA (Visualized Text Embedding for Universal Multi-Modal Retrieval) is a model that aims to enhance text embeddings with visual information, effectively bridging the modality gap.

**How VISTA Works:**

- **Visual Token Embedding**: Introduces visual tokens derived from document images into the text embedding space.
- **Extension of Text Encoders**: Enhances pre-trained text models (like BERT) by adding a visual component.
- **Multi-Stage Training**:
  - **Alignment Phase**: Aligns visual tokens with textual tokens using a large corpus.
  - **Fine-Tuning Phase**: Trains on composed image-text data for multimodal representation capability.

**Strengths:**

- **Improved Retrieval Accuracy**: Enhances understanding of documents with embedded images.
- **Compatibility**: Works with existing text encoders.
- **Efficiency**: Avoids the need for extensive retraining.

**Limitations:**

- **Dependency on Visual Quality**: Performance may degrade with low-quality images.
- **Complexity in Token Integration**: Requires careful balancing to prevent one modality from dominating.

### ColPali: Bridging the Modality Gap with Vision-Language Models

<div class="row mt-3">
{% include figure.html path="assets/img/2025-04-28-MVS/colpali.png" class="img-fluid"%}
</div>
<div class="caption">
    Figure 4: ColPali's unified embedding space for text and images.
</div>

**Overview:**

ColPali is a state-of-the-art multimodal retrieval model that leverages Vision-Language Models (VLMs) to create a unified embedding space for text and images.

**How ColPali Works:**

- **Vision-Language Integration**: Uses a dual-encoder architecture with separate encoders for text and images.
- **Late Interaction Mechanism**: Retains individual token embeddings for richer interactions.
- **Elimination of OCR Bottleneck**: Processes images directly, capturing both textual and visual information without OCR.

**Strengths:**

- **Effective Modality Bridging**: Creates a shared space for all modalities.
- **Enhanced Retrieval Performance**: Excels in retrieving documents with complex layouts.
- **Interpretability**: Allows for analyzing which tokens contribute most to the retrieval score.

**Limitations:**

- **Computational Demands**: Training VLMs is resource-intensive.
- **Data Requirements**: Requires large amounts of multimodal data.
- **Potential Overfitting**: May overfit to specific layouts or styles.

## Interpretability of ColPali in Documents

### Importance of Interpretability

Understanding how a model like ColPali makes retrieval decisions is crucial, especially in sensitive domains like finance.

- **Transparency**: Builds trust by showing which parts of a document influence retrieval.
- **Debugging**: Helps in identifying and correcting model errors.
- **Compliance**: Necessary for legal requirements in regulated industries.

### Case Studies

#### Example 1: Alibaba's 10-K Report

- **Query**: "What are the artificial intelligence tools being developed?"
- **Results**: ColPali successfully retrieved relevant pages, highlighting sections mentioning "AI".

<div class="row mt-3">
{% include figure.html path="assets/img/2025-04-28-MVS/AI_heat_map.png" class="img-fluid" %}
</div>
<div class="caption">
    Figure 5: Heatmaps overlaid on Alibaba's 10-K report highlighting "artificial" and "intelligence".
</div>

#### Example 2: Royal Bank of Canada's Annual Report

- **Query**: "Describe the elements of stock-based compensation."
- **Results**: Retrieved pages discussing stock-based compensation, highlighting relevant terms and tables.

<div class="row mt-3">
{% include figure.html path="assets/img/2025-04-28-MVS/stock_heat_map.png" class="img-fluid" %}
</div>
<div class="caption">
    Figure 6: Heatmaps highlighting relevant terms "stock" and "compensation" in RBC's annual report.
</div>

**Observations:**

- **Effective Highlighting**: Accurately identified relevant text and visual elements.
- **Semantic Understanding**: Recognized synonyms and related terms.

## Innovative Evaluation Methods

### Feature Search Experiments

**Objective:**

Assess how effectively models generate embeddings that capture similarities between documents across modalities.

**Methodology:**

1. **Embedding Extraction**: Used models like **BGE-M3**, **VISTA**, **ColPali**, and **ColQwen**.
2. **Similarity Computation**: Calculated cosine similarity between document embeddings.
3. **Evaluation Metrics**: Employed metrics such as **Precision@K**, **Recall@K**, **MRR**, **MAP**, and **nDCG**.

**Results:**

- **Text-Based Models**: High precision when text is the differentiator but struggled with visual nuances.
- **Multimodal Models**: Outperformed in tasks where visual context is important, effectively capturing both textual and visual similarities.

### Feature Similarity Experiments

**Objective:**

Evaluate the usefulness of embeddings in differentiating between various document types.

**Methodology:**

1. **Prototype Vectors Generation**: Created prototype vectors for each document category (e.g., invoices, contracts, reports) representing the "center" of the embedding space for that class.
2. **Similarity with Cluster Centers**: Compared new document embeddings to these prototype vectors using cosine similarity, classifying each document into the category with the highest similarity score.
3. **Evaluation Metrics**: Used metrics such as **Accuracy** and **Recall** to assess how well the embeddings captured similarities within categories and differences between them.

**Findings:**

- **Text-Based Models**: Effectively categorized documents with distinct textual content.
- **Multimodal Models**: Performed better when visual and layout information were crucial for classification, highlighting their ability to capture complex document features.

<div class="row mt-3">
{% include figure.html path="assets/img/2025-04-28-distill-example/tsne-documents.png" class="img-fluid" %}
</div>
<div class="caption">
    **Figure 7:** t-SNE plot of document embeddings colored by category.
</div>

<!-- ### Clustering-Based Benchmarking Approach

**Methodology:**

1. **Embedding Generation**: Converted documents into embeddings using the evaluated models.
2. **Clustering with HDBSCAN**: Grouped documents based on embedding similarity using the Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) algorithm.
3. **Manual Validation**: Ensured clusters accurately represented document types or topics through manual inspection.
4. **Creation of Retrieval Tasks**: Used cluster representatives as queries to simulate realistic retrieval scenarios.

**Advantages:**

- **Data-Driven Ground Truth**: Reflects actual data distributions without relying on predefined labels.
- **Scalability**: Suitable for large datasets, allowing comprehensive evaluation.
- **Customizable**: Can focus on specific document types or features, enabling targeted benchmarking. -->

## Experimental Evaluations

To evaluate the performance of multimodal retrieval models, we utilized the **ViDoRe Benchmark**, a comprehensive collection designed for assessing document retrieval using visual features. This benchmark includes datasets formatted in a Question-Answering (QA) style to simulate realistic retrieval scenarios.

These datasets encompass a wide range of document types, including financial reports, legal documents, academic papers, manuals, and healthcare records. Each dataset presents unique challenges due to varying content complexity, layouts, and modality combinations.

**Performance Comparison:**

The table below summarizes the performance of various models across the ViDoRe datasets, measured by the Normalized Discounted Cumulative Gain at rank 5 (NDCG@5).

| **Model**                 | **Average** | **TAT-DQA** | **Shift Project** | **Artificial Intelligence** | **Government Reports** | **ArxivQA** | **DocVQA** | **Healthcare Industry** | **InfoVQA** | **Energy** | **TabFQuad** |
|---------------------------|-------------|-------------|-------------------|-----------------------------|------------------------|-------------|------------|-------------------------|-------------|------------|--------------|
| **ColQwen2**              | **89.3**    | 81.4        | 90.7              | 99.4                        | 96.3                   | 88.1        | 60.6       | 98.1                    | 92.6        | 95.9       | 89.5         |
| **ColPali**               | 81.3        | 65.8        | 73.2              | 96.2                        | 92.7                   | 79.1        | 54.4       | 94.4                    | 81.8        | 91.0       | 83.9         |
| **VISTA**                 | 70.8        | 56.9        | 78.6              | 86.8                        | 89.3                   | 39.4        | 32.2       | 91.1                    | 75.0        | 87.7       | 71.2         |
| **E5-Large** | 65.0        | 51.0        | 61.1              | 87.9                        | 84.8                   | 34.0        | 27.8       | 85.5                    | 63.5        | 81.6       | 73.1         |
| **BGE-M3**                | 67.0        | 43.8        | 73.1              | 88.8                        | 80.4                   | 35.7        | 32.9       | 91.3                    | 71.9        | 83.3       | 69.1         |
| **BM25**                  | 65.5        | 62.7        | 64.3              | 92.8                        | 83.9                   | 31.6        | 36.8       | 87.2                    | 62.9        | 85.9       | 46.5         |
| **SigLIP**                | 51.4        | 26.2        | 18.7              | 62.5                        | 66.1                   | 43.2        | 30.3       | 79.1                    | 64.1        | 65.7       | 58.1         |
| **Jina-CLIP**             | 17.7        | 3.3         | 3.8               | 15.2                        | 21.4                   | 25.4        | 11.9       | 20.8                    | 35.5        | 19.7       | 20.2         |

**Observations:**

- **Top Performer:** The **ColQwen2** model achieved the highest average NDCG@5 score of **89.3**, outperforming other models across most datasets.
  
- **Strong Multimodal Performance:** **VISTA** and **ColPali** performed well across multiple datasets, showing robust results on specific domain datasets like **Healthcare Industry** and **Artificial Intelligence**.
  
- **Text vs. Vision Models:** Traditional text-based models such as **BGE-M3** and **BM25** showed competitive results on certain datasets but generally lagged behind multimodal models. Vision-only models like **SigLIP** and **Jina-CLIP** struggled significantly, especially on text-heavy datasets.

- **Dataset Variability:** The performance variance across datasets indicates that some models are better suited for specific domains. For instance, **ColQwen2** excelled in **Government Reports** (96.3) and **Energy** (95.9), suggesting robustness in handling domain-specific jargon and layouts.

## Conclusion

The shift from unimodal to multimodal approaches in document retrieval is revolutionizing how we access complex information. Models like **VISTA**, **ColPali**, and **ColQwen2** not only bridge the modality gap but also set new benchmarks for performance across diverse and complex datasets.

**Key Takeaways:**

1. **Multimodal Models Excel**: Incorporating both textual and visual features significantly improves performance.
2. **Advanced Models Address Modality Challenges**: Models like ColPali and ColQwen provide practical solutions to the modality gap.
3. **Importance of Domain Adaptation**: High-performing models adapt well to different document types and domains.
4. **Interpretability Matters**: Crucial for trust and compliance, especially in sensitive fields like finance.
5. **Innovative Evaluation is Crucial**: New benchmarking strategies enable effective assessment and highlight the strengths of multimodal models.

*The modality gap isn't just being bridged—it's being obliterated.*

## Future Directions

### Technical Advancements

- **Model Efficiency**: Developing models with lower computational costs.
- **Dynamic Context Handling**: Enhancing models to better understand context across document sections.
- **Few-Shot Learning**: Improving generalization from limited examples.

### Interpretability Enhancements

- **Fine-Grained Explanations**: Detailed explanations at the token or image patch level.
- **User-Friendly Tools**: Interfaces that make interpretability accessible.

### Benchmark and Dataset Development

- **Standardized Benchmarks**: Creating widely accepted benchmarks.
- **Open Datasets**: Encouraging the release of diverse datasets.

### User Experience Enhancements

- **Interactive Retrieval Systems**: Interfaces that allow users to guide the retrieval process.
- **Personalization**: Tailoring retrieval results based on user preferences.

*"The future is bright, and it's multimodal."*

---
<!-- ## References

1. Zhang, Y., et al. (2023). ColPali: Bridging the Modality Gap with Vision-Language Models. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.
2. Chen, X., et al. (2023). VISTA: Vision-Augmented Text Embeddings for Multimodal Retrieval. *International Conference on Machine Learning*.
3. Campello, R. J., et al. (2013). Density-Based Clustering Based on Hierarchical Density Estimates. *Advances in Knowledge Discovery and Data Mining*, 160-172.
4. He, K., et al. (2016). Deep Residual Learning for Image Recognition. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 770-778.
5. Vaswani, A., et al. (2017). Attention Is All You Need. *Advances in Neural Information Processing Systems*.
6. Zhou, J., Li, W., Wang, X., et al. (2024). VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval. *arXiv preprint arXiv:2406.04292*.
7. Faysse, M., Barlas, G., Saleem, H., Piwowarski, B., & Doucet, A. (2024). ColPali: Efficient Document Retrieval with Vision Language Models. *arXiv preprint arXiv:2407.01449*.
8. Liang, W., Li, X., Ktena, S. I., O'Dea, S., et al. (2022). Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning. *arXiv preprint arXiv:2203.02053*.
9. Chen, J., Xiao, S., Zhang, P., Luo, K., Lian, D., & Liu, Z. (2024). BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. *arXiv preprint arXiv:2402.03216*.
10. Wang, S., Yu, J., Zhang, H., Dai, Z., Liu, Z., & Sun, M. (2022). Text Embeddings by Weakly-Supervised Contrastive Pre-training. *arXiv preprint arXiv:2204.08455*.
11. Xiao, S., Chen, J., Zhang, P., Luo, K., Lian, D., & Liu, Z. (2023). BGE: Bilingual Generative Enhanced Embeddings for Cross-Lingual Retrieval. *arXiv preprint arXiv:2301.00322*.
12. Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. *arXiv preprint arXiv:1908.10084*.
13. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. *arXiv preprint arXiv:2103.00020*.
14. Khattab, O., & Zaharia, M. (2020). ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. *arXiv preprint arXiv:2004.12832*.
15. Gao, T., Yao, X., & Chen, D. (2021). SimCSE: Simple Contrastive Learning of Sentence Embeddings. *arXiv preprint arXiv:2104.08821*.
16. H. Leal. (2023). Interpretability of ColPali in Financial Documents. *Medium*. Retrieved from [https://medium.com/@hlealpablo/interpretability-of-colpali-in-financial-documents-5a2dcdeeba3a](https://medium.com/@hlealpablo/interpretability-of-colpali-in-financial-documents-5a2dcdeeba3a). -->

**Nikhil Reddy** is a researcher at Mila, Quebec, specializing in machine learning, document analysis, and information retrieval systems.
