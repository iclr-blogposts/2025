---
layout: distill
title: Distilling LLMs into MiniLLMs
description: "In this blog post, we propose to investigate a technique to distill LLMs into smaller models. Relying on the paper <em>MiniLLM: Knowledge Distillation of Large Language Models</em>, published in March 2024 by Yuxian Gu et al., we summarize and discuss its key findings. We supplement their work by reproducing their results and explore its generalizability."
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Param Damle
    url: "https://app.paramdamle.com/about"
    affiliations:
      name: Carngie Mellon University
  - name: Clement Ou
    url: "https://clementou.com/"
    affiliations:
      name: Carngie Mellon University
  - name: Ines Vignal
    url: "https://www.linkedin.com/in/ines-vignal/"
    affiliations:
      name: Carngie Mellon University

# must be the exact same name as your blogpost
bibliography: 2025-04-28-miniLLM.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
    subsections:
    - name: Subsection

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## MiniLLM: Knowledge Distillation of Large Language Models

### Background

**Large Language Models (LLMs)** have witnessed a surge in computational demand as they've grown in size. TODO add stats about models growing and computational size.
Techniques to reduce this computational load include **Knowledge Distillation (KD)**, in which a "smaller" student model is fine-tuned using data or feedback from a larger "teacher" model. TODO cite KD.

Types of KD include<d-cite key="Gou2021KDSurvey"></d-cite>:
- **Black-box KD** only exposes the input-output pairs from the teacher to the student. This has shown promising results with prompt-response pairs from prominent LLM APIs, which most researchers can only query external APIs for, without any access to internal state. Prominent examples include Stanford's Alpaca<d-cite key="Alpaca"></d-cite> and Vicuna<d-cite key="Vicuna2023"></d-cite>, both of which use training data generated from queries to major LLMs.
- **White-box KD** exposes the internals of a teacher model (e.g. hidden state data, a full distribution over predicted outputs) that allows student models to gain more insight into the full decision-making process of the teacher, rather than just its final verdict. The authors argue this method has only been applied to language _understanding_<d-cite key="Sanh2020DistilBERT"></d-cite><d-cite key="Wang2020MiniLM"></d-cite> (tasks that require interpreting pre-generated text) rather than _generation_ (an inherently more creative and open-ended task). TODO assess if there's really no other similar paper, otherwise a comparison is in order.

### Intention
As LLMs are increasingly used to generate new content and the research community output more open source models, there is an opportunity to train smaller models using white-box knowledge of well-performing generators.
The intention of this paper is to optimize _white-box_ KD for _generative_ tasks. The central focuses include:
1. Distill larger teacher model into a smaller student model.
2. Generate relevant and generalizable responses to a prompt.
3. Disincentivize outputting obscure or nonsensical answers.
4. Maintain truthfulness (relevant to the teacher distribution).
5. Disincentivize brevity in responses.

### Comparing Models
With white-box access to the output distribution of the teacher model, _how well a student model has distilled knowledge_ can be measured by _how closely the student model's output distribution aligns with the teacher's_.

Let's say the teacher LLM's probability distribution over text output, $P$, closely resembles that of real documents and human-written text. Now, we want to train the parameters $\theta$ of our small student model so it outputs a distribution $Q_\theta$ to best mimic $P$. One standard measure of how different these two distributions are is **(forward) KL Divergence**:
$$D_{\text{KL}}(P||Q_\theta) = \sum_{x\in \mathcal{X}} P(x) \log\left(\frac{P(x)}{Q_\theta(x)}\right)$$
Note the asymmetric definition&mdash;in other words, $D_{\text{KL}}(P||Q_\theta) \neq D_{\text{KL}}(Q_\theta||P)$. So what does this asymmetry mean in practice?

Say the distribution $P$ has many **modes** (the probability mass function looks hilly with several outputs that have a higher probability than most other outputs). An example of this is how an LLM is _far_ more likely to spit out a small set of outputs (sentences that make sense in English) compared to the vast majority of _possible_ outputs (all random sequences of words).

When we try to adjust distribution $Q_\theta$ to minimize the divergence $D_{\text{KL}}(P||Q_\theta)$, we try to get it to increase the probability it assigns to each of $P$'s modes. This may work fine with a small, finite set of modes, such as in an understanding or classification task where we limit the possible set of class labels or insights a model can draw from an input so that we can better evaluate whether it's extracting the _proper_ understanding.

What about text generation? Think about how many modes would exist in a distribution over all possible valid outputs a model could generate given some prompt. Because forward KL wants to boost probabilities in $Q_\theta$, it dislikes lowering the probabilities of any output ("**zero-avoiding**"<d-cite key="Malinin2019ReverseKL"></d-cite>), so attempting to lower $D_{\text{KL}}(P||Q_\theta)$ results in a distribution for $Q_\theta$ that spreads out across all possible outputs with not much precision for identifying the modes of $P$. This is especially limiting if our $Q_\theta$ distribution is output by a student model whose small size limits its capacity to express large, complicated distributions.

In such cases, the authors argue a worthy tradeoff is having a $Q_\theta$ that boosts the probabilities of _some_ of the modes in $P$, and diminishes the probability of all the other outputs, even if there's some other, less significant modes among the noisy garbage outputs that get silenced. The **Reverse KL Divergence** strategy argues that KD should minimize $D_{\text{KL}}(Q_\theta||P)$ instead. This objective will boost the probabilities in $Q_\theta$ associated with the biggest modes of $P$, focusing on capturing the most frequent outputs and pushing all other probabilities down ("**zero-forcing**"). Diminishing the random, noisy, or garbage output is core to ensuring the output of $Q_\theta$, and thereby our student model, is free of hallucinations (TODO cite hallucination problem) or otherwise nonsensical content.

The differences between the result of minimizing different objectives is shown elegantly in this graph:
TODO attach Figure 2

But wait&mdash;won't the student model lose the slightly smaller modes? In other words, won't our student model fail to learn more obscure but equally valid responses from our teacher distribution? The authors argue that:
1. They don't care about capturing all, or even multiple, valid responses; simply _one_ correct response from a student model in response to a prompt is sufficient.
2. A student model minimizing reverse KL still achieved linguistically complex and diverse responses (TODO make sense of 4 gram result from Table 2)
3. A student model minimizing reverse KL is still able to output the full distribution of text from the source distribution (TODO make sense of loss result from Table 2)

### Experimental Setup

If using forward KL Divergence, a **sequence-level KD** approach would take a prompt and feed it through both the student and teacher model. It would keep sampling outputs from the resulting teacher distribution to get an understanding of $P$, and extract the raw distribution $Q_\theta$ from the student to calculate the loss $\mathcal{L}(\theta) = D_{\text{KL}}(P||Q_\theta)$, then propagate the gradient $\nabla\mathcal{L}(\theta)$ back to the student so it can update its model weights $\theta$ to lower the divergence on the next iteration.

With white-box access to the teacher model, we can now poll the _student_'s output distribution to mimic $Q_\theta$, the frequency of responses from the student, and compare it with the true value of $P$ extracted from the teacher to calculate $\nabla\mathcal{L}(\theta) = \nabla D_{\text{KL}}(Q_\theta||P)$. In intuitive terms, this means instead of _increasing the likelihood the student outputs the full range of samples frequently outputted by the teacher_, we aim to _train the student to output samples that resonate with the teacher's preference/distribution_.

Policy Gradient
increase conditional p to maximize alignment with teacher but keep q low to allow probability space to be diversified by other options
Policy gradient suffers from reward hacking, TODO https://openreview.net/pdf?id=yb3HOXO3lX2
Making loss gradient go to 0 means Rt go to 0, which is most favorable with no tokens at all!

1. single step decomposition to reduce variance: single step generation quality of token at t' is critical to training variance, error in front tokens accumulates to latter tokens. extracts a term that is computable relative to parameters instead of doing monte carlo sampling (TODO summing over vocabulary expensive?) apparently reduces variance and accelerates convergence
This means less spikes in reverse KLD every step
2. teacher mixed smapling to alleviate reward hacking: e.g. repeated or degenerate phrases. mix teacher and student distribution weighted avg with alpha = 0.2. more alpha is more teacher mix in. suppresses low quality generation because the loss is sampled with a combo of teacher and student distributions, and evaluated on the student's share (multiplied with a different factor to avoid variances accumulating)
RKLD decreases more without this, but generated sequences are short, meaningless repeated strings (Figure 8)
3. length normalization to eliminate length bias: long sequences have low R, or divergence from here onward? (intuition, p will have lower prob than q, so multiplying a bunch of fractions yields a fraction, log of which is very negative) which produces very large positive delta loss, and model wants to move to 0. basically make it the log geometric mean, or arithmetic mean of log

KD previously applied to white box or small model to imitate black box mirroring ChatGPT
distill knowledge of lang gen white box models
exigence: open source LLMs on the rise? TODO cite
replace common forward KLD with reverse KLD, more suitable for lang gen to prevent student from overestimating low probability regions

train the student model on large long document corpus to get sensibility. fine tune model on a text generation task dataset D. compute loss of single and long with clipping for stability? TODO https://arxiv.org/pdf/1707.06347

add language modeling loss to penalize repeating its foundational learning

Methods: various models, 120M -> 13B size, evaluated on 5 datasets (list them?) split by response length

compare with big models. OpenWeb for GPT2 and RoBERTa training for other models

Eval:
RougeL: precision for alrge scale instruction following TODO https://aclanthology.org/W04-1013.pdf
human judgement: ask humans to pick a winner between 2 outputs
GPT4 evaluation: ask GPT to compare

SFT: fine tune directly on D supervised by TODO golden responses?
KD: fine tune on D using teacher distribution on token by token
SeqKD: student model fine tuned on data generated by teacher ? TODO specify

Results: good generality
most precise alignment with ground truth, better than teacher
Teacher forcing, training inference discrepancy, exposure bias
MiniLLM improves consistently regardless of scale
better human preference

student model may not always proportionally scale with teacher size due to what? TODO https://arxiv.org/pdf/1902.03393

Exposure bias due to difference between teacher forcing training and free run generation TODO 
avoided since student sees its own responses in training

Models trained with policy optimization are poorly calibrated TODO https://cdn.openai.com/papers/gpt-4.pdf
Test on classification tasks, with zero shot classification instructions

Differences in length show that MiniLLM outperforms regardless of golden length range, with no clear pattern
poor performance in ground truth < 5 tokens due to lack of representation in data
intuition: more tokens = bigger space = more nodes = better fit for reverse KL


metrics:
1. higher overall quality
2. lower exposure bias
3. better calibration
4. higher long text generation performance

Method scalable from 120M to 13B parameters