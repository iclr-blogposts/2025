---
layout: distill
title: Linear Recursions for Everyone
description: Your blog post's abstract.
  Please add your abstract or summary here and not in the main body of your text. 
  Do not include math/latex or hyperlinks.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
authors:
  - name: Anonymous

# must be the exact same name as your blogpost
bibliography: 2025-04-28-linrec.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
    subsections:
    - name: Motivation
    - name: Mamba with a Linear Recursion
  - name: Part I - Algorithm
    subsections:
    - name: Parallel Calculation
    - name: Gradient Calculation
  - name: Part II - Implementation

---

## Introduction
### Motivation

With the publication of Mamba<d-cite key="gu2024mamba"></d-cite> in late 2023, a particular algorithm received once again the attentation of the Deep Learning community<d-footnote> e.g. Tweets by <a href="https://x.com/Algomancer/status/1740171408284782966">Adam Hibble</a>, <a href="https://x.com/francoisfleuret/status/1788489112283996367">Francois Fleuret</a> or <a href="https://github.com/pytorch/pytorch/issues/95408#issuecomment-2327232046">PyTorch Issues</a></d-footnote>. 
The so-called parallel scan<d-cite key="ladner1980parallelprefix"></d-cite><d-cite key="blelloch1990prefixapplication"></d-cite> allows to parallelize the inherently sequential operation of a scan if its update function is associative. This can be used to compute cumulative sums and products or certain types or recurrent neural network (RNN) layers<d-cite key="martin2018parallelizing"></d-cite>. In particular, it is a fundamental building block of an emerging class of architectures inspired by state space models (SSMs) such as S4<d-cite key="gu2024mamba"></d-cite>, S5<d-cite key="smith2023s5"></d-cite>, LRU<d-cite key="orvieto2023lru"></d-cite>, or Mamba<d-cite key="gu2024mamba"></d-cite>. These models show promising results on a wide range of tasks, while exhibiting linear runtime complexity $O(L)$ in the sequence lenght $L$. But unfortunately, investigating them can be challanging due to an inaccessibility of the parallel scan; it cannot be expressed efficiently in PyTorch and its implementation is often hidden in CUDA Kernels. This leads to misconceptions and further inhibits the accessibility of this important research area.

In this article, we aim to lead the reader to the topic using the abstraction of a linear recursion

$$ 
y_l =  y_{l-1} \cdot c_l + x_l
$$

with inputs $$ x_l $$, coefficients $$ c_l $$ and outputs $$ y_l $$. In contrast to  other great resources explaining the parallel scan and the individual models themselves<d-cite key="rush2022annotatedS4"></d-cite><d-cite key="chen2024mamba"></d-cite><d-cite key="grootendorst2024mamba"></d-cite><d-cite key="rush2024mamba"></d-cite>, we focus on the computational structure which is common accross all SSM or diagonal linear RNN based models. This further allows us to reach a rather simple parallel description of the linear recursion algorithm, ignoring all the complexities of the tree formulation of a more general parallel scan.


Coincidentally, this algorithmic description can be mapped very efficiently onto the CUDA architecture<d-cite key="merrill2016cubscan"></d-cite>. So in the second part of the article, we will gradually work towards a simple CUDA implementation in the form of a PyTorch extension. As a side-effect to the educative aspect of the exercise, the code can be used as a template for easy prototyping and research. It allows to express Mamba in terms of 'unfused', primitive operations in PyTorch, much like matrix multiplications can express Flash-Attention<d-footnote> e.g. in Code by <a href=" https://github.com/CLAIRE-Labo/flash_attention">Caglar Glucehre</a></d-footnote>. Finally, we will learn that the runtime of the parallel linear recursion is practically as fast as a binary vector operation (e.g. `torch.add`). 

All the code will be made available (is available at anonymous repo?)


### Mamba with a Linear Recursion
Let's first convince ourselves that we can express Mamba, more precisely it's sequence mixing layer, with a linear recursion. To start, we express the linear recursion with a simple loop: 

<details>
<summary>Show code: minimal imports</summary>
{% highlight python %}
import torch
from torch import nn
from einops import rearrange, repeat, einsum
{% endhighlight %}
</details>
{% highlight python %}
# linear recursion y_i = y_{i-1} * c_i + x_i
def linrec(inputs:torch.Tensor, coeffs:torch.Tensor):
    outputs = torch.zeros_like(inputs)
    prev = torch.zeros_like(outputs[..., 0])
    for i in range(0, inputs.shape[-1]):
        outputs[..., i] = prev * coeffs[..., i] + inputs[..., i]
        prev = outputs[..., i].clone()
    return outputs
{% endhighlight %}


Note that the code for this section is also available in this [Google Colab](https://colab.research.google.com/drive/1Fk4tpzHluKFSLrLdcvWXiZwMEOd_3BtS). To continue, we dissect [`mamba_simple.py`](https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py) by the original authors. Since there are a lot of moving parts, we focus on the call to [`selective_scan_fn`](https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py) and how its arguments are prepared. In short, there is an input-independent model parameter `A` and a projection layer `in_proj`, which maps an input `u` to `x` and input-dependent parameters `B`, `C`, and `dt`. Then, the selective scan performs some reparametrizations, expands `x` with `B` into an inner dimension, computes the linear scan of `x` with coefficients `A`, and projects the result with `C` back to the shape of `x`:


<details>
<summary>Show code: define model parameters</summary>
{% highlight python %}
# model params from
# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py
d_model = 1024                          # W: width of u (default: d_model=1024 in mamba1-370m)
expand  = 2                             # expansion from d_state to d_inner
d_inner = expand * d_model              # D: width of x (default: expand=2 => d_inner=2048)
d_state = 64                            # N: width of one SSM-head  (default: d_state=64)
ngroups = 1                             # G: number heads that share B and C projection vectors
assert(d_inner % ngroups == 0)
{% endhighlight %}
</details>


<details>
<summary>Show code: prepare dummy data</summary>
{% highlight python %}
# prepare dummy data according to
# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py
device = torch.device('cuda:0')
dtype = torch.float32
batchsize, seqlen = 1, 2**10

# A is only input independent param
A = torch.rand((d_inner, d_state), device=device, dtype=dtype) * 15 + 1
A = A = -torch.exp(A.log().float()) # for completeness
in_proj = nn.Linear(d_model, d_inner + d_inner + ngroups*d_state + ngroups*d_state + d_inner, device=device, dtype=dtype)

# prepare input u and input-dependent params 
u = torch.randn((batchsize, seqlen, d_model), device=device, dtype=dtype)
_, x, B, C, dt = torch.split(in_proj(u), [d_inner, d_inner, ngroups*d_state, ngroups*d_state, d_inner], dim=-1)
B = rearrange(B, 'B L (G N) -> B G N L', G=ngroups, N=d_state)
C = rearrange(C, 'B L (G N) -> B G N L', G=ngroups, N=d_state)
x = rearrange(x, 'B L D -> B D L')
dt = rearrange(dt, 'B L D -> B D L')
dt = nn.functional.softplus(dt) # map to positive range
{% endhighlight %}
</details>


{% highlight python %}
# selective S6 scan based on linear recursion
def selective_scan_linrec(x:torch.Tensor, dt:torch.Tensor, A:torch.Tensor, B:torch.Tensor, C:torch.Tensor) -> torch.Tensor:
    # prepare A, B, dt of size (B=batch, D=d_inner, N=d_state, L=seqlen)
    A = repeat(A, 'D N -> B D N L', B=batchsize, L=seqlen)
    B = repeat(B, 'B G N L -> B (G x) N L', x=d_inner//ngroups)
    C = repeat(C, 'B G N L -> B (G x) N L', x=d_inner//ngroups)
    dt = repeat(dt, 'B D L -> B D N L', N=d_state)

    # reparameterize A, B
    A = torch.exp(A * dt)
    B = B * dt

    # expand scalars x with vectors B to vectors x in inner dimension
    x = einsum(x, B, 'B D L, B D N L -> B D N L')

    # compute linear recursion in inner dimension
    y = linrec(inputs=x, coeffs=A)

    # project vectors h in inner dimension with vectors C to scalars y
    y = einsum(C, y, 'B D N L, B D N L -> B D L')
    return y
{% endhighlight %}


Finally, we test `selective_scan_linrec` by comparing with two reference implementations:
{% highlight python %}
# selective scan reference implementations
from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref
y_linrec = selective_scan_linrec(x, dt, A, B, C)          
y_sol = selective_scan_fn(u=x, delta=dt, A=A, B=B, C=C)   # error: 7.629e-06
y_ref = selective_scan_ref(u=x, delta=dt, A=A, B=B, C=C)  # error: 3.815e-06
{% endhighlight %}

This illustrates how linear recursions are the building block of Mamba, and it can be expanded to many other architectures such as S4<d-cite key="gu2024mamba"></d-cite>, S5<d-cite key="smith2023s5"></d-cite>, LRU<d-cite key="orvieto2023lru"></d-cite> and even Mamba-2<d-cite key="dao2024mamba2"></d-cite>. In the special case of linear time-invariant (LTI) systems such as S4<d-cite key="gu2024mamba"></d-cite>, the coefficient `coeff` would be shared across sequence length. Note that in practice, the projection, reparametrization and state expansion are fused into the linear recursion. This makes the algorithm hardware-aware and drastically increases runtime by reducing memory accesses.


## Part I - Algorithm

Let's begin again with the linear recursion $$y_l =  y_{l-1} \cdot c_l + x_l$$ of inputs $$x_l$$, coefficients $$c_l$$ and outputs $$y_l$$ for steps $$l=0 \ldots L-1$$ and starting at $$y_0=x_0$$. Unrolling the recursion yields an equivalent formulation of a weighted sum

$$
y_l 
= \sum_{k=0}^{l} 
\underbrace{(\prod_{i=k+1}^{l} c_i)}_{=\tilde{c}_{k,l}} \cdot x_k
= \sum_{k=0}^{l} \tilde{c}_{k,l} \cdot x_k,
$$

where $$\tilde{c}_{k,l}$$ are cumulative coefficients from $$k$$ to $$l$$ and $$\tilde{c}_{l,l}=1$$. If we consider sequences $$x=[x_{k}]_{k=0}^{L'-1}$$, $$c=[c_{i}]_{i=0}^{L'-1}$$, and $$y=[y_{l}]_{l=0}^{L'-1}$$, we can describe the linear recursion as a linear sequence mixer $$y = f(x,c) = \tilde{C}^T x$$. The mixing matrix $$\tilde{C}^T$$ is lower triangular, where the diagonal is a sequence of ones, the subdiagonal is the sequence $$c$$ and a lower diagonal entry at index $$l,k$$ contains the cumulative product $$\tilde{c}_{k,l}$$. In the special case of a single shared coefficient $$c_l=z \in [0,1]$$, the function $$f$$ is an exponential moving average filter, an instance of a convolutional operator, and $$\tilde{C}$$ an instance of a circulant matrix. This allows parallelization via the fast Fourier transform which was used in the original S4 implementation, but is limited to this special case. As we will see, the parallel scan is also based on a divide-and-conquer approach but works for arbitrary $$c$$ and runs in $$O(L)$$ instead of $$O(L \text{log} L)$$.

### Parallel Calculation
We approach the divide-and-conquer algorithm from the bottom up. To compute a linear recurrence on two threads, we split the sequences at $$L'$$ into two parts. The first thread simply computes the linear recursion $$[y_{l}]_{l=0}^{L'-1}$$ up to element $$L'$$. To see how the second thread avoids performing the same sequential computation, we decompose $$y_l$$ into two sums 

$$
y_l = 
\underbrace{
\Big(\sum_{k=0}^{L'-1} \tilde{c}_{k,L'-1} \cdot x_k \Big) 
}_{= y_{L'-1}}
\cdot \tilde{c}_{L'-1,l} +
\underbrace{
\sum_{k=L'}^{l} \tilde{c}_{k,l} \cdot x_k
}_{= \bar{y}_{L',l}}
\qquad \text{for}\ l \geq L'.
$$

Note that $$\bar{y}_{L',l}$$ corresponds to a linear recursion starting at $$L'$$ up to $$l$$. This means that the second thread can compute the recurrence $$[\bar{y}_{L',l}]_{l=L'}^{L-1}$$ independently and store the cumulative coefficients $$[\tilde{c}_{L'-1,l}]_{l=L'}^{L-1}$$ as a by-product. Finally, the second thread receives $$y_{L'-1}$$ from the first and combines the terms as $$ [y_{l}]_{l=L'}^{L-1} = y_{L'-1} \cdot [\tilde{c}_{L'-1,l}]_{l=L'}^{L-1} + [\bar{y}_{L',l}]_{l=L'}^{L-1}$$ where $$\cdot$$ and $$+$$ act element-wise. In PyTorch pseudo code, this would corresponds to:
{% highlight python %}
y[..., :Lp] = linrec(x[..., :Lp], c[..., :Lp]) # thread 1
y[..., Lp:] = linrec(x[..., Lp:], c[..., Lp:]) # thread 2
y[..., Lp:] += y[..., Lp-1] * torch.cumprod(c[..., Lp:], dim=-1) # thread 2
{% endhighlight %}


Now, the attentive reader might have noticed that the second thread still has to perform $$O(L)$$ operations, so what did we gain? In the setting of $$T$$ threads, every thread has to perform $$O(L/T)$$ operations sequentially, then all threads aggregate the cumulative sum of transition elements with a $$O(\text{log} T)$$ sequential overhead, and finally the threads combine the result in $$O(L/T)$$. This results in an overall algorithmic complexity of $$O(L/T + \text{log} T)$$ for $$T$$ threads.


### Gradient Calculation

In order to implement $$f(x,c)$$ in an auto-grad system such as PyTorch, we need to implement a `backward` function which back-propagates the gradient $$\delta^{(y)}:=\frac{\partial \mathcal{L}}{\partial y}^T$$ through the linear recurrence and returns $$\delta^{(x)}:= \frac{\partial\mathcal{L}}{\partial x}^T =$$ and $$\delta^{(c)}:= \frac{\partial \mathcal{L}}{\partial c}^T$$. We will now calculate the vector-Jacobian-products $${\delta^{(x)}}^T=\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial x}$$ and $${\delta^{(c)}}^T=\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial c}$$ as derived from the chain rule:


1. Let us first consider the derivative of an output $y_l$ with respect to an input $$x_k$$
   
   $$\frac{d y_l}{d x_k} = 
     \begin{cases} 
     \tilde{c}_{k,l} &\text{if}\ k \leq l \\
     0 &\text{if}\ l < k
     \end{cases} 
   $$
   
   Inserting the derivative into $$\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial x}$$, we again observe the structure of the weighted sum. 
  
   $$
   \delta_k^{(x)}
   = \sum_{l=0}^{L-1} \delta_l^{(y)} \cdot \frac{d y_l}{d x_k}
   = \sum_{l=k}^{L-1} \tilde{c}_{k, l} \cdot \delta_l^{(y)}
   $$

   Rearranging the terms, we unroll $$\delta^{(x)}$$ into a reversed linear recursive form 

   $$
   \delta_k^{(x)} = \delta_{k+1}^{(x)} \cdot c_{k+1} + \delta_k^{(y)}
   $$

2. Let us now consider the derivative of an output $$y_l$$ with respect to a coefficient $$c_i$$. We observe that $$c_i$$ and $$x_k$$ only interact with $$y_l$$ if $$k < i \leq l$$  and therefore 

   $$ 
    \frac{d y_l}{d c_i} = 
    \begin{cases}
    \displaystyle \sum_{k=0}^{i-1} (\prod_{j=k+1}^{i-1} c_j)(\prod_{j=i+1}^{l} c_j) \cdot x_k
    = y_{i-1} \cdot \tilde{c}_{i, l} &\text{if}\ i \leq l \\
    0 &\text{if}\ l < i
    \end{cases}
   $$

   Inserting the derivative into $$\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial c}$$, we again observe the structure of the weighted sum, i.e.

   $$
   \delta_i^{(c)}
   = \sum_{l=0}^{L-1} \delta_l^{(y)} \cdot \frac{d y_l}{d c_i}
   = \sum_{l=i}^{L-1}  y_{i-1} \cdot \tilde{c}_{i,l} \cdot \delta_l^{(y)}
   $$

   Rearranging the terms, we express $$\delta^{(c)}$$ as a function of the known $y$ and $$\delta^{(x)}$$

   $$
   \delta_i^{(c)} = y_{i-1} \cdot \delta_{i}^{(x)} 
   $$


This provides a very simple way to write a `backward` function in PyTorch. The only requirements are a shift function and a `forward` function which also runs in reverse. 
<details>
<summary>Show code: `shift` and `linrec_ref_fwd` functions</summary>
{% highlight python %}
def linrec_ref_fwd(inputs:torch.Tensor, coeffs:torch.Tensor, reverse=False):
    outputs = torch.zeros_like(inputs)
    prev = torch.zeros_like(outputs[..., 0])
    for i in range(0, inputs.shape[-1])[::-1 if reverse else 1]:
        outputs[..., i] = prev * coeffs[..., i] + inputs[..., i]
        prev = outputs[..., i].clone()
    return outputs

def shift(input, shifts, fillval=0): # torch.roll without the copy of the wrap-around section
    if shifts > 0:
        output = torch.cat([torch.full_like(input[..., :shifts], fillval), input[..., :-shifts]], dim=-1)
    if shifts < 0:
        output = torch.cat([input[..., -shifts:], torch.full_like(input[..., shifts:], fillval)], dim=-1)
    return output
{% endhighlight %}
</details>


{% highlight python %}
def linrec_ref_bwd(d_outputs:torch.Tensor, coeffs:torch.Tensor, outputs:torch.Tensor, reverse=False):
    coeffs = shift(coeffs, -1 if not reverse else 1, fillval=0)
    d_inputs = linrec_ref_fwd(inputs=d_outputs, coeffs=coeffs, reverse=(not reverse))
    d_coeffs =  d_inputs * shift(outputs, shifts=1 if not reverse else -1, fillval=0)
    return d_inputs, d_coeffs
{% endhighlight %}



But wait, in Mamba the `coeffs` are input-dependent parameters! Fortunately, this case is automatically handeled by `torch.autograd` via the multi-variable chain rule. In this special case, $$x=z$$ and $$c=g(z)$$ depend on a common input $$z$$ and appling the chain rule yields

$$
\newcommand{\L}{\mathcal{L}} 
{\delta^{(z)}}^T
:= \frac{\partial\L}{\partial z}
= \frac{\partial \L}{\partial y} \frac{\partial y}{\partial z}
= \frac{\partial \L}{\partial y} \Big(
\frac{\partial y}{\partial x} \frac{\partial x}{\partial z} + 
\frac{\partial y}{\partial c} \frac{\partial c}{\partial z}
\Big)
=  {\delta^{(x)}}^T + {\delta^{(c)}}^T \frac{\partial c}{\partial z} .
$$  

The situation is similar for S4 where $$c=z_0$$ depends on a single recurrent coefficient $$z_0$$

$$
\delta^{(z)} 
:= \frac{d \L}{d z_0}
= \frac{\partial \L}{\partial y} \frac{\partial y}{\partial c} \frac{\partial c}{\partial z_0}
= {\delta^{(c)}}^T \frac{\partial c}{\partial z_0}   = \sum \delta_{i}^{(c)}.
$$

PyTorch will derive those cases from the `backward` function of the linear recursion.


## Part II - Implementation

