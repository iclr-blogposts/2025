---
layout: distill
title: Linear Recursions for Everyone
description: Investigating linear RNNs such as Mamba, can be challenging because they are currently not efficiently expressible in PyTorch. We propose the abstraction of linear recursions to gain intuition for the computational structure of these emerging deep learning architectures. After deriving their parallel algorithm, we gradually build towards a simple template CUDA extension for PyTorch. We hope that making linear recursions accessible to a wider audience inspires further research on linear-time sequence mixing.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
authors:
  - name: Anonymous

# must be the exact same name as your blogpost
bibliography: 2025-04-28-linrec.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
    subsections:
    - name: Motivation
    - name: Mamba with a Linear Recursion
  - name: Part I - Algorithm
    subsections:
    - name: Parallel Calculation
    - name: Gradient Calculation
  - name: Part II - Implementation
    subsections:
    - name: Setup
    - name: Parallel Scan
    - name: Limitations

styles: >
  .callout-block {
      width: 85%;
      margin: 25px auto;           
      padding: 15px 30px;          
      background-color: #1f2937;
      color: #f3f4f6;
      border-radius: 8px;    
  }
  .note {
    margin-bottom: 10px; 
    margin-top: 10px; 
    overflow: hidden; 
    color: #31708f; 
    background-color: #d9edf7; 
    border-color: #bce8f1; 
    padding: 15px; border: 1px solid transparent; 
    border-radius: 4px;
  }
  .blocktip {
    background: #bbb;
    border-left: 2px solid var(--global-theme-color);
    margin: 1.5em 10px;
    padding: 0.5em 10px;
    font-size: 1.1rem;
  }
  .block-tip {
      border-color: var(--global-tip-block);
      background-color: var(--global-tip-block-bg);
      p {
        color: var(--global-tip-block-text);
      }
      h1,
      h2,
      h3,
      h4,
      h5,
      h6 {
        color: var(--global-tip-block-title);
      }
    }
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Introduction
### Motivation

With the publication of Mamba<d-cite key="gu2024mamba"></d-cite> in late 2023, a particular algorithm received once again the attentation of the Deep Learning community<d-footnote> e.g. Tweets by <a href="https://x.com/Algomancer/status/1740171408284782966">Adam Hibble</a>, <a href="https://x.com/francoisfleuret/status/1788489112283996367">Francois Fleuret</a> or <a href="https://github.com/pytorch/pytorch/issues/95408#issuecomment-2327232046">PyTorch Issues</a></d-footnote>. 
The so-called parallel scan<d-cite key="ladner1980parallelprefix"></d-cite><d-cite key="blelloch1990prefixapplication"></d-cite> allows to parallelize the inherently sequential operation of a scan if its update function is associative. This can be used to compute cumulative sums and products, or certain types or recurrent neural network (RNN) layers<d-cite key="martin2018parallelizing"></d-cite>. In particular, it is a fundamental building block of an emerging class of architectures inspired by state space models (SSMs) such as S4<d-cite key="gu2024mamba"></d-cite>, S5<d-cite key="smith2023s5"></d-cite>, LRU<d-cite key="orvieto2023lru"></d-cite>, or Mamba<d-cite key="gu2024mamba"></d-cite>. These models show promising results on a wide range of tasks, while exhibiting linear runtime complexity $O(L)$ in the sequence length $L$. But unfortunately, investigating them can be challanging because the parallel scan cannot be expressed efficiently in PyTorch and its implementation is often hidden in CUDA Kernels. This leads to misconceptions and further inhibits the accessibility of this important research area.

In this article, we aim to lead the reader to the topic using the abstraction of a linear recursion

$$ 
y_l =  y_{l-1} \cdot c_l + x_l
$$

with inputs $$ x_l $$, coefficients $$ c_l $$ and outputs $$ y_l $$. In contrast to  other great resources explaining the parallel scan and the individual models themselves<d-cite key="rush2022annotatedS4"></d-cite><d-cite key="chen2024mamba"></d-cite><d-cite key="grootendorst2024mamba"></d-cite><d-cite key="rush2024mamba"></d-cite>, we focus on the computational structure which is common across all SSM or diagonal linear RNN based models. This further allows us to reach a rather simple parallel description of the linear recursion algorithm, ignoring all the complexities of the tree formulation of a more general parallel scan.


Coincidentally, this algorithmic description can be mapped very efficiently onto the CUDA architecture<d-cite key="merrill2016cubscan"></d-cite>. So in the second part of the article, we will gradually work towards a simple CUDA implementation in the form of a PyTorch extension. As a side-effect to the educative aspect of the exercise, the code can be used as a template for easy prototyping and research. It allows to express Mamba in terms of 'unfused', primitive operations in PyTorch, much like matrix multiplications can express Flash-Attention<d-footnote> e.g. in Code by <a href="https://github.com/CLAIRE-Labo/flash_attention">Caglar Glucehre</a></d-footnote>. Finally, we will learn that the runtime of the parallel linear recursion is practically as fast as a binary vector operation (e.g. `torch.add`). 

All the code is available as an anonymous repository [here](https://anonymous.4open.science/r/linrec-2F7F).


### Mamba with a Linear Recursion
Let's first convince ourselves that we can express Mamba, more precisely its sequence mixing layer, with a linear recursion. To start, we express the linear recursion with a simple loop: 

<details>
<summary>Show code: minimal imports</summary>
{% highlight python %}
import torch
from torch import nn
from einops import rearrange, repeat, einsum
{% endhighlight %}
</details>
{% highlight python %}
# linear recursion y_i = y_{i-1} * c_i + x_i
def linrec(inputs:torch.Tensor, coeffs:torch.Tensor):
    outputs = torch.zeros_like(inputs)
    prev = torch.zeros_like(outputs[..., 0])
    for i in range(0, inputs.shape[-1]):
        outputs[..., i] = prev * coeffs[..., i] + inputs[..., i]
        prev = outputs[..., i].clone()
    return outputs
{% endhighlight %}


Note that the code for this section is also available in this [Google Colab](https://colab.research.google.com/drive/1Fk4tpzHluKFSLrLdcvWXiZwMEOd_3BtS). To continue, we dissect [`mamba_simple.py`](https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py) by the original authors. Since there are a lot of moving parts, we focus on the call to [`selective_scan_fn`](https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py) and how its arguments are prepared. In short, there is an input-independent model parameter `A` and a projection layer `in_proj`, which maps an input `u` to `x` and input-dependent parameters `B`, `C`, and `dt`. Then, the selective scan performs some reparametrizations, expands `x` with `B` into an inner dimension, computes the linear scan of `x` with coefficients `A`, and projects the result with `C` back to the shape of `x`:


<details>
<summary>Show code: define model parameters</summary>
{% highlight python %}
# model params from
# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py
d_model = 1024                          # W: width of u (default: d_model=1024 in mamba1-370m)
expand  = 2                             # expansion from d_state to d_inner
d_inner = expand * d_model              # D: width of x (default: expand=2 => d_inner=2048)
d_state = 64                            # N: width of one SSM-head  (default: d_state=64)
ngroups = 1                             # G: number heads that share B and C projection vectors
assert(d_inner % ngroups == 0)
{% endhighlight %}
</details>


<details>
<summary>Show code: prepare dummy data</summary>
{% highlight python %}
# prepare dummy data according to
# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py
device = torch.device('cuda:0')
dtype = torch.float32
batchsize, seqlen = 1, 2**10

# A is only input independent param
A = torch.rand((d_inner, d_state), device=device, dtype=dtype) * 15 + 1
A = A = -torch.exp(A.log().float()) # for completeness
in_proj = nn.Linear(d_model, d_inner + d_inner + ngroups*d_state + ngroups*d_state + d_inner, device=device, dtype=dtype)

# prepare input u and input-dependent params 
u = torch.randn((batchsize, seqlen, d_model), device=device, dtype=dtype)
_, x, B, C, dt = torch.split(in_proj(u), [d_inner, d_inner, ngroups*d_state, ngroups*d_state, d_inner], dim=-1)
B = rearrange(B, 'B L (G N) -> B G N L', G=ngroups, N=d_state)
C = rearrange(C, 'B L (G N) -> B G N L', G=ngroups, N=d_state)
x = rearrange(x, 'B L D -> B D L')
dt = rearrange(dt, 'B L D -> B D L')
dt = nn.functional.softplus(dt) # map to positive range
{% endhighlight %}
</details>


{% highlight python %}
# selective S6 scan based on linear recursion
def selective_scan_linrec(x:torch.Tensor, dt:torch.Tensor, A:torch.Tensor, B:torch.Tensor, C:torch.Tensor) -> torch.Tensor:
    # prepare A, B, dt of size (B=batch, D=d_inner, N=d_state, L=seqlen)
    A = repeat(A, 'D N -> B D N L', B=batchsize, L=seqlen)
    B = repeat(B, 'B G N L -> B (G x) N L', x=d_inner//ngroups)
    C = repeat(C, 'B G N L -> B (G x) N L', x=d_inner//ngroups)
    dt = repeat(dt, 'B D L -> B D N L', N=d_state)

    # reparameterize A, B
    A = torch.exp(A * dt)
    B = B * dt

    # expand scalars x with vectors B to vectors x in inner dimension
    x = einsum(x, B, 'B D L, B D N L -> B D N L')

    # compute linear recursion in inner dimension
    y = linrec(inputs=x, coeffs=A)

    # project vectors h in inner dimension with vectors C to scalars y
    y = einsum(C, y, 'B D N L, B D N L -> B D L')
    return y
{% endhighlight %}


Finally, we test `selective_scan_linrec` by comparing with two reference implementations:
{% highlight python %}
# selective scan reference implementations
from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref
y_linrec = selective_scan_linrec(x, dt, A, B, C)          
y_sol = selective_scan_fn(u=x, delta=dt, A=A, B=B, C=C)   # error: 7.629e-06
y_ref = selective_scan_ref(u=x, delta=dt, A=A, B=B, C=C)  # error: 3.815e-06
{% endhighlight %}

This illustrates how linear recursions are the building block of Mamba, and it can be expanded to many other architectures such as S4<d-cite key="gu2024mamba"></d-cite>, S5<d-cite key="smith2023s5"></d-cite>, LRU<d-cite key="orvieto2023lru"></d-cite> and even Mamba-2<d-cite key="dao2024mamba2"></d-cite>. In the special case of linear time-invariant (LTI) systems such as S4<d-cite key="gu2024mamba"></d-cite>, the coefficient `coeff` would be shared across sequence length. Note that in practice, the projection, reparametrization and state expansion are fused into the linear recursion. This makes the algorithm hardware-aware and drastically increases runtime by reducing memory accesses.


## Part I - Algorithm

In the previous section, we learned how the rather simple PyTorch function `linrec` can express the sequence mixing component of SSMs or linear RNNs such as Mamba. Unfortunately, this formulation is inhibitively slow even for toy problems. In this section, we will establish further intuitions for the linear recursion, how to paralellize it, and how to calculate its gradients. 

Let's begin by defining the linear recursion $$y_l =  y_{l-1} \cdot c_l + x_l$$ of inputs $$x_l$$, coefficients $$c_l$$ and outputs $$y_l$$ starting at $$y_0=x_0$$ and iterating for $$l=0 \ldots L-1$$ steps. By unrolling the recursion, we obtain an equivalent formulation of a weighted sum

$$
y_l 
= \sum_{k=0}^{l} 
\underbrace{(\prod_{i=k+1}^{l} c_i)}_{=\tilde{c}_{k,l}} \cdot x_k
= \sum_{k=0}^{l} \tilde{c}_{k,l} \cdot x_k,
$$

where $$\tilde{c}_{k,l}$$ are cumulative coefficients from $$k$$ to $$l$$ and $$\tilde{c}_{l,l}=1$$. If we consider sequences $$x=[x_{k}]_{k=0}^{L'-1}$$, $$c=[c_{i}]_{i=0}^{L'-1}$$, and $$y=[y_{l}]_{l=0}^{L'-1}$$, we can describe the linear recursion as a linear sequence mixer $$y = f(x,c) = \tilde{C}^T x$$. The mixing matrix $$\tilde{C}^T$$ is lower triangular, where the diagonal is a sequence of ones, the subdiagonal is the sequence $$c$$ and a lower diagonal entry at index $$l,k$$ contains the cumulative product $$\tilde{c}_{k,l}$$. In the special case of a single shared coefficient $$c_l=z \in [0,1]$$, the function $$f$$ is an exponential moving average filter, an instance of a convolutional operator, and $$\tilde{C}$$ an instance of a circulant matrix. This allows parallelization via the fast Fourier transform which was used in the original S4 implementation, but is limited to this special case. As we will see, the parallel scan is also based on a divide-and-conquer approach but works for arbitrary $$c$$ and runs in $$O(L)$$ instead of $$O(L \text{log} L)$$.

### Parallel Calculation
We approach the divide-and-conquer algorithm from the bottom up. To compute a linear recursion on two threads, we split the sequences at $$L'$$ into two parts. The first thread simply computes the linear recursion $$[y_{l}]_{l=0}^{L'-1}$$ up to element $$L'$$. To see how the second thread avoids performing the same sequential computation, we decompose $$y_l$$ into two sums 

$$
y_l = 
\underbrace{
\Big(\sum_{k=0}^{L'-1} \tilde{c}_{k,L'-1} \cdot x_k \Big) 
}_{= y_{L'-1}}
\cdot \tilde{c}_{L'-1,l} +
\underbrace{
\sum_{k=L'}^{l} \tilde{c}_{k,l} \cdot x_k
}_{= \bar{y}_{L',l}}
\qquad \text{for}\ l \geq L'.
$$

Note that $$\bar{y}_{L',l}$$ corresponds to a linear recursion starting at $$L'$$ up to $$l$$. This means that the second thread can compute the recursion $$[\bar{y}_{L',l}]_{l=L'}^{L-1}$$ independently and store the cumulative coefficients $$[\tilde{c}_{L'-1,l}]_{l=L'}^{L-1}$$ as a by-product. Finally, the second thread receives $$y_{L'-1}$$ from the first and combines the terms as $$ [y_{l}]_{l=L'}^{L-1} = y_{L'-1} \cdot [\tilde{c}_{L'-1,l}]_{l=L'}^{L-1} + [\bar{y}_{L',l}]_{l=L'}^{L-1}$$ where $$\cdot$$ and $$+$$ act element-wise. In PyTorch pseudo code, this would corresponds to:
{% highlight python %}
y[..., :Lp] = linrec(x[..., :Lp], c[..., :Lp]) # thread 1
y[..., Lp:] = linrec(x[..., Lp:], c[..., Lp:]) # thread 2
y[..., Lp:] += y[..., Lp-1] * torch.cumprod(c[..., Lp:], dim=-1) # thread 2
{% endhighlight %}


Now, the attentive reader might have noticed that the second thread still has to perform $$O(L)$$ operations, so what did we gain? In the setting of $$T$$ threads, every thread has to perform $$O(L/T)$$ operations sequentially, then all threads communicate the transition elements with a $$O(\text{log} T)$$ sequential overhead, and finally the threads combine the result in $$O(L/T)$$. This results in an overall algorithmic complexity of $$O(L/T + \text{log} T)$$ for $$T$$ threads.


### Gradient Calculation

In order to implement $$f(x,c)$$ in an auto-grad system such as PyTorch, we need to implement a `backward` function which back-propagates the gradient $$\delta^{(y)}:=\frac{\partial \mathcal{L}}{\partial y}^T$$ through the linear recursion and returns $$\delta^{(x)}:= \frac{\partial\mathcal{L}}{\partial x}^T =$$ and $$\delta^{(c)}:= \frac{\partial \mathcal{L}}{\partial c}^T$$. We will now calculate the vector-Jacobian-products $${\delta^{(x)}}^T=\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial x}$$ and $${\delta^{(c)}}^T=\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial c}$$ as derived from the chain rule:


1. Let us first consider the derivative of an output $y_l$ with respect to an input $$x_k$$
   
   $$\frac{d y_l}{d x_k} = 
     \begin{cases} 
     \tilde{c}_{k,l} &\text{if}\ k \leq l \\
     0 &\text{if}\ l < k
     \end{cases} 
   $$
   
   Inserting the derivative into $$\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial x}$$, we again observe the structure of the weighted sum. 
  
   $$
   \delta_k^{(x)}
   = \sum_{l=0}^{L-1} \delta_l^{(y)} \cdot \frac{d y_l}{d x_k}
   = \sum_{l=k}^{L-1} \tilde{c}_{k, l} \cdot \delta_l^{(y)}
   $$

   Rearranging the terms, we unroll $$\delta^{(x)}$$ into a reversed linear recursive form 

   $$
   \delta_k^{(x)} = \delta_{k+1}^{(x)} \cdot c_{k+1} + \delta_k^{(y)}
   $$

2. Let us now consider the derivative of an output $$y_l$$ with respect to a coefficient $$c_i$$. We observe that $$c_i$$ and $$x_k$$ only interact with $$y_l$$ if $$k < i \leq l$$  and therefore 

   $$ 
    \frac{d y_l}{d c_i} = 
    \begin{cases}
    \displaystyle \sum_{k=0}^{i-1} (\prod_{j=k+1}^{i-1} c_j)(\prod_{j=i+1}^{l} c_j) \cdot x_k
    = y_{i-1} \cdot \tilde{c}_{i, l} &\text{if}\ i \leq l \\
    0 &\text{if}\ l < i
    \end{cases}
   $$

   Inserting the derivative into $$\frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial c}$$, we again observe the structure of the weighted sum, i.e.

   $$
   \delta_i^{(c)}
   = \sum_{l=0}^{L-1} \delta_l^{(y)} \cdot \frac{d y_l}{d c_i}
   = \sum_{l=i}^{L-1}  y_{i-1} \cdot \tilde{c}_{i,l} \cdot \delta_l^{(y)}
   $$

   Rearranging the terms, we express $$\delta^{(c)}$$ as a function of the known $y$ and $$\delta^{(x)}$$

   $$
   \delta_i^{(c)} = y_{i-1} \cdot \delta_{i}^{(x)} 
   $$


This provides a very simple way to write a `backward` function in PyTorch. The only requirements are a shift function and a `forward` function with support for recursion in reverse direction. 
<details>
<summary>Show code: `shift` and `linrec_ref_fwd` functions</summary>
{% highlight python %}
def linrec_ref_fwd(inputs:torch.Tensor, coeffs:torch.Tensor, reverse=False):
    outputs = torch.zeros_like(inputs)
    prev = torch.zeros_like(outputs[..., 0])
    for i in range(0, inputs.shape[-1])[::-1 if reverse else 1]:
        outputs[..., i] = prev * coeffs[..., i] + inputs[..., i]
        prev = outputs[..., i].clone()
    return outputs

def shift(input, shifts, fillval=0): # torch.roll without the copy of the wrap-around section
    if shifts > 0:
        output = torch.cat([torch.full_like(input[..., :shifts], fillval), input[..., :-shifts]], dim=-1)
    if shifts < 0:
        output = torch.cat([input[..., -shifts:], torch.full_like(input[..., shifts:], fillval)], dim=-1)
    return output
{% endhighlight %}
</details>


{% highlight python %}
def linrec_ref_bwd(d_outputs:torch.Tensor, coeffs:torch.Tensor, outputs:torch.Tensor, reverse=False):
    coeffs = shift(coeffs, -1 if not reverse else 1, fillval=0)
    d_inputs = linrec_ref_fwd(inputs=d_outputs, coeffs=coeffs, reverse=(not reverse))
    d_coeffs =  d_inputs * shift(outputs, shifts=1 if not reverse else -1, fillval=0)
    return d_inputs, d_coeffs
{% endhighlight %}



But wait, in Mamba the `coeffs` are input-dependent parameters! Fortunately, this case is automatically handeled by `torch.autograd` via the multi-variable chain rule. In this special case, $$x=z$$ and $$c=g(z)$$ depend on a common input $$z$$ and appling the chain rule yields

$$
\newcommand{\L}{\mathcal{L}} 
{\delta^{(z)}}^T
:= \frac{\partial\L}{\partial z}
= \frac{\partial \L}{\partial y} \frac{\partial y}{\partial z}
= \frac{\partial \L}{\partial y} \Big(
\frac{\partial y}{\partial x} \frac{\partial x}{\partial z} + 
\frac{\partial y}{\partial c} \frac{\partial c}{\partial z}
\Big)
=  {\delta^{(x)}}^T + {\delta^{(c)}}^T \frac{\partial c}{\partial z} .
$$  

The situation is similar for S4 where $$c=z_0$$ depends on a single recurrent coefficient $$z_0$$

$$
{\delta^{(z)}}^T
:= \frac{d \L}{d z_0}
= \frac{\partial \L}{\partial y} \frac{\partial y}{\partial c} \frac{\partial c}{\partial z_0}
= {\delta^{(c)}}^T \frac{\partial c}{\partial z_0}   = \sum \delta_{i}^{(c)}.
$$

PyTorch will derive those cases from the `backward` function of the linear recursion.


## Part II - Implementation

In the previous sections, we learned how to express the backward function `linrec_ref_bwd` in terms of its forward function `linrec_ref_fwd` and we gained some intution on how the latter could be parallelized. But as the reader might be aware, long for-loops in PyTorch still represent a serious barrier to efficient code, in particular if they need to be compiled. Nevertheless, there exist implementations such as [`mamba.py`](https://github.com/alxndrTL/mamba.py)<d-cite key="mambapy"></d-cite> which apply the devide-and-conquer approach to express the scan in PyTorch. This is called a device-wide scan and requires the execution of many independent sub-operations, so-called _kernels_, on an GPU. To avoid this overhead, we would prefer to fuse the entire linear scan into a single kernel. But this is not possible because PyTorch currently does not provide a way to express for-loops which are executed in one kernel. In this section, we learn how to express the linear recursion first as a for-loop and then as a parallel scan in the GPU programming language CUDA.

The goal of this chapter is to familiarize the reader with the basic CUDA programming model in the context of the linear recursion. Many resources such as the [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide), the [CUDA C++ Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide), or [the GPU Mode Lectures](https://github.com/gpu-mode) venture very quickly into the intricacies of high-performance optimization for expert users. There exist already quite a few implementations of a parallel scan <d-cite key="merrill2016cubscan"></d-cite><d-cite key="johnryan265pscan"></d-cite><d-cite key="mambapy"></d-cite><d-cite key="acceleratedscan"></d-cite>. Here however, we aim to lower the entry bar for PyTorch users and provide an intuition for the computational structure of the problem. The code for Part II is available in this anonymous repo [here](https://anonymous.4open.science/r/linrec-2F7F).


### Preparing a CUDA Extension for PyTorch

Although there are a few resources on how to write PyTorch extensions such as the tutorials [Custom C++ and CUDA Extensions](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [Custom C++ and CUDA Operators](https://pytorch.org/tutorials/advanced/cpp_custom_ops.html), the learning curve can be quite steep at times. Therefore, we will aim to provide a rough sketch on what is needed to get started. More in-depth explanations are generally available in the repository. We begin by installing the newest compatible combination of PyTorch, CUDA and, GCC<d-footnote> more information in the <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#host-compiler-support-policy">CUDA Installation Guide</a></d-footnote>.

{% highlight shell %}
conda create -n CUDA12.4 -c conda-forge gxx==13.2 python==3.12 nvidia::cuda==12.4
conda activate CUDA12.4
pip install numpy pandas matplotlib ninja torch==2.5
{% endhighlight %}

Now, we can write a function using the [C++ frontend of PyTorch](https://pytorch.org/cppdocs/), which might look a bit familiar. Since we want to call our function from within Python, the entry point into the code will not be a classical `main()` function. Instead, we compile the code to a shared library (`.so`) and load it dynamically into the Python runtime. This is conveniently handled by PyTorch and pybind.
{% highlight cpp %}
#include <torch/torch.h>
#include <torch/extension.h>
#include <pybind11/pybind11.h>

using torch::Tensor;
Tensor linrec_ref_fwd(const Tensor &inputs, const Tensor &coeffs, const bool reverse) {
    Tensor outputs = torch::empty_like(inputs);
    // do something
    return outputs;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("linrec_ref_fwd", wrap_pybind_function(linrec_ref_fwd), 
              "Reference CUDA imlplementation of linear recursion forward pass.",
              py::arg("inputs"), py::arg("coeffs"), py::arg("reverse")=false);
}
{% endhighlight %}


In `build.py`, we define a build configuration and call `torch.utils.cpp_extension.load`, which in turn invokes `nvcc` and `gcc` and finally loads the pybind module into the Python runtime.

<details>
<summary>Show code: build.py </summary>
{% highlight python %}
import os, shutil
from pathlib import Path
from torch.utils.cpp_extension import load
SRC_DIR = Path(__file__).parent
BUILD_DIR = SRC_DIR / ".build"

### CUDA/C++ Compilation Arguments
EXT_SOURCES = [str(SRC_DIR / "extension.cu")]
INCLUDES = [str(SRC_DIR)] + CUDA_RUNTIME_INCLUDES

CUDA_FLAGS = [
    # Options for specifying behavior of compiler/linker.
    "--generate-line-info",                     # Generate line-number information for device code.
    "-std=c++20",                               # Select a particular C++ dialect
    # Options for passing specific phase options
    # -Xptxas: options for ptxas, the PTX optimizing assembler.
    "-Xptxas", "-warn-spills",                  # Warning if registers are spilled to local memory.
    "-Xptxas", "-warn-lmem-usage",              # Warning if local memory is used.
    # Miscellaneous options for guiding the compiler driver
    "--keep",                                   # Keep all intermediate files that are generated during internal compilation steps.
    # Options for steering GPU code generation.
    "--use_fast_math",                          # Make use of fast math library.
    # Generic tool options.
    "--source-in-ptx",                          # Interleave source in PTX. May only be used in conjunction with --device-debug or --generate-line-info.
    "--resource-usage",                         # Show resource usage such as registers and memory of the GPU code. Implies '--ptxas-options --verbose'.
]
CPP_FLAGS = ["-std=c++20"]

def make_build_dir(clean=False):
    if clean:
        shutil.rmtree(BUILD_DIR, ignore_errors=True)
    os.makedirs(BUILD_DIR, exist_ok=True)
    
def extension(extra_cflags=[], extra_cuda_cflags=[], verbose=False, clean=False):
    make_build_dir(clean=clean)
    ext = load(
        name="pylinrec",
        sources=EXT_SOURCES,
        extra_include_paths=INCLUDES,
        extra_cflags=CPP_FLAGS + extra_cflags,
        extra_cuda_cflags=CUDA_FLAGS + extra_cuda_cflags,
        build_directory=str(BUILD_DIR),
        verbose=verbose)
    return ext
{% endhighlight %}
</details>


{% highlight python %}
>>> import torch
>>> import build
>>> _C = build.extension()  # compiles and loads .build/pylinrec.so as a module
>>> _C.linrec_ref_fwd(torch.Tensor(10), torch.Tensor(10))
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) # empty outputs
{% endhighlight %}


### Reference Implementation

With the build pipeline in place, we translate the reference implementation to CUDA. In the figure below, we see that a GPU consits of many small compute-units, so-called Shared Multiprocessors or SMs. With a slight oversimplification, SMs execute independent blocks of computation. To partition the work among these blocks, we have to consider the computational structure of our `linrec_ref_fwd` function. Note that the `inputs` and `coeffs` tensors are stored as one flattend array in memory and their their last dimension represents the sequence index. This means that consecutive sequence elements are also consecutive in memory. Therefore, the $$i$$-th block can simply process the $$i$$-th sequence at the memory index `seqLen*blockIdx.x`.

<div class="row mt-3">
    <div class="col-xl mt-3 mt-xl-0">
      {% include figure.html path="assets/img/2025-04-28-linrec/GA100-arch.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    The Nvidia A100 GPU Architecture. Source:
    <a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=20"> NVIDIA A100 White Paper</a>
</div>

With this insight, we can easily implement `linrec_ref_fwd_kernel` with a parallel for-loop, where each block just computes the linear recursion of its assigned sequence. Recall that the function `linrec_ref_fwd` from the previous section is invoked from within Python and runs on the CPU. To launch the kernel on the GPU, it schedules one block for each sequence. The number of squences is determined by the number of batches and channels.

<details>
<summary>Show code: kernel launch from `linrec_ref_fwd` </summary>
{% highlight cpp %}
Tensor linrec_ref_fwd(const Tensor &inputs, const Tensor &coeffs) {
    // Input checks: matching dimensions, strides, devices, dtype
    Tensor outputs = torch::empty_like(inputs); // prepare outputs

    // Infer number of sequences and sequence length
    TORCH_CHECK(inputs.stride(-1) == 1);        // inner most dimension is last
    const int seqlen = inputs.size(-1);         // the sequence length
    const int numseq = inputs.numel() / seqlen; // the number of sequences: batches*channels*...

    // Launch kernel function
    const int threads = 1; 
    const int blocks = numseq;
    linrec_ref_fwd_kernel<float><<<blocks, threads>>>(inputs.data_ptr<float>(), 
        coeffs.data_ptr<float>(), outputs.data_ptr<float>(), seqlen
    );
    return outputs;
}

{% endhighlight %}
</details>


{% highlight cpp %}
template <typename kT>
__global__ void linrec_ref_fwd_kernel(const kT* inputs, const kT* coeffs, kT* outputs, const int seqLen) {
    // Layout: dim=(numseq,seqLen), strides=(seqLen,1)
    int seqBaseIdx = seqLen * blockIdx.x; // threads block process sequences independently: inputs[seqBaseIdx + i]
    inputs = &inputs[seqBaseIdx];         // get pointer to sequence
    coeffs = &coeffs[seqBaseIdx];         // get pointer to sequence
    outputs = &outputs[seqBaseIdx];       // get pointer to sequence

    // Linear Recursion
    outputs[0] = inputs[0];                         // set start element
    for(int i = 1; i < seqLen; i++) {               // linear scan
        outputs[i] = outputs[i-1] * coeffs[i] + inputs[i];
    }
}
{% endhighlight %}

Note that the kernel is a _templated_ C++ function taking a datatype `kT` as compile-time parameter. This allows to instanciate the kernel for the different required datatypes and is a common theme in CUDA programs. More generally, templating allows to select and dispatch any compile-time configuration at runtime using the `static constexpr` feature from C++20.

<details>
<summary>Show code: the dispatch function </summary>
{% highlight cpp %}
template <std::array CONFIG_LIST, std::size_t I = 0>
inline void dispatch(auto config, auto &&func, std::string funcname) {
    static constexpr auto CONFIG = CONFIG_LIST[I];
    if (CONFIG == config) {
        func.template operator()<CONFIG>();  // call with compile-time config
        return;
    }
    if constexpr (I + 1 < CONFIG_LIST.size()) {
        dispatch<CONFIG_LIST, I + 1>(config, func, funcname);
        return;
    }
    TORCH_CHECK_NOT_IMPLEMENTED(false, "'", funcname, "' is not compiled for this config.")
}
{% endhighlight %}
</details>



{% highlight cpp %}
static constexpr auto CONFIG_LIST = std::array{/*config1*/, /*config2*/, ...};
dispatch<CONFIG_LIST>(config, [&]<auto config>() {
    mytemplatedfunc</*config*/><<<blocks, threads>>>( 
        inputs.data_ptr<T>(),
        outputs.data_ptr<T>(),
    );
}, "mytemplatedfunc"); // name for errors
{% endhighlight %}


### Tile Implementation
In the previous section, each block of work executed just a single thread sequentially. But zooming in on the Shared Multiprocessor, we see that a block can execute operations on up to 1024 threads in synchronization! Furthermore, the SM has a total of 65536 registers which means that one block can hold and process a tile containing two `inputs` and `coeffs` sequences up to length 32768 at once. How can we efficiently make use of all these resources?

<div class="row justify-content-md-center">
    <div class="col-6 mt-3 mt-sm-0">
      {% include figure.html path="assets/img/2025-04-28-linrec/GA100-sm.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    The Nvidia A100 Shared Multiprocessor. Source:
    <a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=21"> NVIDIA A100 White Paper</a>
</div>

We begin by partitioning the work among blocks in the same way as the reference implementation, i.e. all threads in the same block share the same pointer. Then, we need to distribute the sequence of length `seqLen` among the threads of the block `numThreads`. If they are not divisible, there will be a tail of threads which are assigned a shorter `threadSeqLen`. Note that all threads execute the same code, but self-assign different base indices and sequence lengths depending on their `threadId`. Finally, they load the respective subsequences of `inputs` and `coeffs` into a thread-local array. An important detail here is that the compile-time argument `kMaxElemsPerThread` determines the size and accesses over the thread-local at compile-time. This guarantees that the array is statically mapped to the registers on the SM.

<details>
<summary>Show code: kernel launch from `linrec_tile_fwd` </summary>
{% highlight cpp %}
Tensor linrec_tile_fwd(const Tensor &inputs, const Tensor &coeffs, const Option& options) {
    // Input checks: matching dimensions, strides, devices, dtype
    Tensor outputs = torch::empty_like(inputs); // Prepare Outputs

    // Infer number of sequences and sequence length
    TORCH_CHECK(inputs.stride(-1) == 1);        // inner most dimension is last
    int seqlen = inputs.size(-1);               // the sequence length
    int numseq = inputs.numel() / seqlen;       // the number of sequences over batches, channels, etc

    // Unpack and determine compile-time arguments
    int kMaxElemsPerThread = get(options, "kMaxElemsPerThread", 16); 
    int kMaxThreadsPerWarp = get(options, "kMaxThreadsPerWarp", 32); 
    int kMaxThreadsPerBlock = get(options, "kMaxThreadsPerBlock", 1024); 

    // Dispatch templated function: instantiate compile-time configuration
    static constexpr auto CONFIG_LIST = std::array{/*config1*/, /*config2*/, ...};
    auto config = std::array{kMaxElemsPerThread, kMaxThreadsPerWarp, kMaxThreadsPerBlock};
    dispatch<CONFIG_LIST>(config, [&]<auto config>() {
        // select kernel based on compile-time arguments 
        static constexpr int kMaxElemsPerThread = config[0];
        static constexpr int kMaxThreadsPerWarp = config[1];
        static constexpr int kMaxThreadsPerBlock = config[2];
        auto kernel = linrec_tile_fwd_kernel<float, kMaxElemsPerThread, kMaxThreadsPerWarp, kMaxThreadsPerBlock>;

        // determine run-time arguments
        int blocks = numseq;
        int threads = std::min(ceildiv(seqlen, kMaxElemsPerThread), kMaxThreadsPerBlock);

        // launch kernel
        kernel<<<blocks, threads, smem, stream>>>(inputs.data_ptr<float>(),
            coeffs.data_ptr<float>(), outputs.data_ptr<float>(), seqlen);
        C10_CUDA_KERNEL_LAUNCH_CHECK();
    }, "linrec_tile_fwd_kernel"); // name for errors
    return outputs;
}

{% endhighlight %}
</details>

<div class="l-body-outset">
{% highlight cpp %}
template <typename kT, ushort kMaxElemsPerThread, ushort kMaxThreadsPerWarp, ushort kMaxThreadsPerBlock>
__global__ void __launch_bounds__(kMaxThreadsPerBlock)
linrec_tile_fwd_kernel(const kT* inputs, const kT* coeffs, kT* outputs, int const seqLen) {
    // Layout: dim=(X,L), strides=(L,1)
    const int seqBaseIdx = seqLen * blockIdx.x; // process sequences independently: inputs[seqBaseIdx+i]
    inputs = &inputs[seqBaseIdx];               // get pointer to sequence
    coeffs = &coeffs[seqBaseIdx];               // get pointer to sequence
    outputs = &outputs[seqBaseIdx];             // get pointer to sequence

    // Determine Tile Layout
    const ushort numThreads = blockDim.x;
    const ushort threadId = threadIdx.x;                                                  // index of current thread
    const ushort elemsPerThread = ceildiv(seqLen, (int) numThreads);                      // distribute seqLen among numThreads
    const ushort numTailThreads = numThreads * elemsPerThread - seqLen;                   // last numTailThreads have one elem less
    const int threadTailId = (int) threadId - (numThreads - numTailThreads);              // tail start indicated by ..., 0, 1, 2, ...
    const ushort threadSeqLen = (threadTailId < 0) ? elemsPerThread : (elemsPerThread-1); // sequence length processed by every thread
    const ushort threadBaseIdx = threadId * elemsPerThread - max(threadTailId, 0);        // base index to process by every thread

    // Load inputs and coeffs of tile into thread-local arrays
    kT threadAccOutput[kMaxElemsPerThread];
    kT threadAccCoeff[kMaxElemsPerThread];
    for(ushort i = 0; i < kMaxElemsPerThread; i++) {
        threadAccOutput[i] = (i < threadSeqLen) ? inputs[threadBaseIdx + i] : 0;  // load or fill with 0
        threadAccCoeff[i] = (i < threadSeqLen) ? coeffs[threadBaseIdx + i] : 1;   // load or fill with 1
    }
    // Compute parallel scan on a tile (=subsequence) that fits into one thread block 
    _linrec_scan_tile_parallel_<kT, kMaxElemsPerThread, kMaxThreadsPerWarp, kMaxThreadsPerBlock, algocode>(
        threadAccOutput, threadAccCoeff, numThreads
    );
    for(ushort i = 0; i < threadSeqLen; i++) { // Store outputs
        outputs[threadBaseIdx + i] = threadAccOutput[i];
    }
}
{% endhighlight %}
</div>


Recall the parallel form of the linear recursion from Part I. The second thread calculated the linear recursion $$[\bar{y}_{L',l}]_{l=L'}^{L-1}$$ and stored the cumulative coefficients $$[\tilde{c}_{L'-1,l}]_{l=L'}^{L-1}$$ on its sub-sequence. This is exactly implemented in level 1 of `_linrec_scan_tile_parallel_`. For level 2, we need to introduce the concept of a warp. In CUDA, a warp represents a group of 32 threads which are grouped and executed one common instruction at the time. Therefore, communication between threads in the same warp incurrs no overhead. For example, the `__shfl_up_sync` instruction copies a variable to the thread with `id+delta`. This allows to propagate and accumulate the transition elements across threads, so that every thread receives its offset `warpAccOutput` ($$y_{L'-1}$$) within 6 steps. Once that is achieved, the simple re-combination remains. We need to compute the exact warp size, because it might not be full.

<div class="l-body-outset">
{% highlight cpp %}

template <typename kT, ushort kMaxElemsPerThread, ushort kMaxThreadsPerWarp, ushort kMaxThreadsPerBlock>
__forceinline__  __device__  void _linrec_scan_tile_parallel_(kT* threadAccOutput, kT* threadAccCoeff, const ushort numThreads) {
    // Level 1: Accumulate elements within this thread
    for(ushort i = 1; i < kMaxElemsPerThread; i++) {
        threadAccOutput[i] = threadAccOutput[i-1] * threadAccCoeff[i] + threadAccOutput[i];
        threadAccCoeff[i]  = threadAccCoeff[i-1] * threadAccCoeff[i];
    }
    
    // Level 2: Accumulate elements across threads within this warp
    // Determine Warp Configuration
    const ushort laneId = threadIdx.x % kMaxThreadsPerWarp;
    const ushort warpId = threadIdx.x / kMaxThreadsPerWarp;
    const ushort numWarps = ceildiv(numThreads, kMaxThreadsPerWarp);
    const ushort lastWarpSize = numThreads - kMaxThreadsPerWarp * (numWarps-1);
    const ushort thisWarpSize = (warpId==numWarps-1) ? lastWarpSize : kMaxThreadsPerWarp;
    
    kT warpAccOutput = __shfl_up_sync(0xffffffff, threadAccOutput[kMaxElemsPerThread-1], 1); // get transition elements between threads
    kT warpAccCoeff  = __shfl_up_sync(0xffffffff, threadAccCoeff[kMaxElemsPerThread-1], 1);  // get transition elements between threads
    warpAccOutput = (laneId == 0) ? 0 : warpAccOutput;  // set default 1 for first lane (=thread in warp)
    warpAccCoeff  = (laneId == 0) ? 1 : warpAccCoeff;   // set default 0 for first lane (=thread in warp)
    for (ushort delta = 1; delta < thisWarpSize; delta *= 2) { 
        kT prevAccOutput = __shfl_up_sync(0xffffffff, warpAccOutput, delta);
        kT prevAccCoeff  = __shfl_up_sync(0xffffffff, warpAccCoeff, delta);

        // don't update warpAccOutput and warpAccCoeff in delta lower lanes
        warpAccOutput = (laneId < delta) ? warpAccOutput : prevAccOutput * warpAccCoeff + warpAccOutput;
        warpAccCoeff  = (laneId < delta) ? warpAccCoeff  : prevAccCoeff * warpAccCoeff;
    }

    for (ushort i = 0; i < kMaxElemsPerThread; i++) { // distribute accumulates into thread elements
        threadAccOutput[i] = warpAccOutput * threadAccCoeff[i] + threadAccOutput[i];
    }
}
{% endhighlight %}
</div>

With the upper limit of 1024 per block, propagation of the transition elements across warps is required. This can be achieved by copying the warp-level transition elements into block-shared memory, where the first warp performs the propagation, and finally each thread combines its entries with the offset `blockAccOutput`. We refer to the code, for more details. This means that a linear recursion of length 32768 could be computed with 5*32+20 floating point operations on 1024 threads if all registers could be used for the thread-local arrays. Pretty cool!


### Pipe Implementation

In order to support linear recursions exceeding the maximum tile size, we introduce a `tileBaseIdx` and `tileSeqLen` which allow us to sequentially accumulate across tiles in an outer loop. This requires some minor adjustments to the `threadBaseIdx` and `threadSeqLen` calculation. The kernel now processes tiles in a pipelined manner and it could even be feasible to overlap asynchronous memory loading with actual computation. At this point we would also like to draw the readers attention to the particular choice of `ushort` to represent most indices. Since the `tileSeqLen` is limited by the amount of registers of an SM, we know that all variables in the range of the tile are guaranteed to be smaller than 65536. By using `ushort`, we free make more registers available to be used by the thread-local arrays.

<details>
<summary>Show code: `linrec_pipe_fwd` kernel </summary>.
{% highlight cpp %}
template <typename kT, ushort kMaxElemsPerThread, ushort kMaxThreadsPerWarp, ushort kMaxThreadsPerBlock>
__global__ void __launch_bounds__(kMaxThreadsPerBlock)
linrec_pipe_fwd_kernel(const kT* inputs, const kT* coeffs, kT* outputs, int const seqLen) {
    // Layout: dim=(X,L), strides=(L,1)
    const int seqBaseIdx = seqLen * blockIdx.x; // process sequences independently in reverse: inputs[seqBaseIdx-i]
    inputs = &inputs[seqBaseIdx];               // get pointer to sequence
    coeffs = &coeffs[seqBaseIdx];               // get pointer to sequence
    outputs = &outputs[seqBaseIdx];             // get pointer to sequence

    __shared__ kT seqAccOutput, seqAccCoeff; // for sequential accumulation between tiles
    if (threadIdx.x == 0) {
        seqAccOutput = 0;
        seqAccCoeff = 1;
    } __syncwarp(); // avoid divergence

    // Determine Tile Layout
    const ushort numThreads = blockDim.x;
    const ushort threadId = threadIdx.x;                     // index of current thread
    const ushort elemsPerTile = kMaxElemsPerThread * numThreads;                                        // the default number of elements per tile
    for (int tileBaseIdx = !rev ? 0; tileBaseIdx < seqLen; tileBaseIdx += elemsPerTile) { // linear scan over tiles
        const ushort tileSeqLen = min(seqLen - tileBaseIdx, elemsPerTile);                              // length of the tile to scan with thread block
        const ushort elemsPerThread = ceildiv(tileSeqLen, numThreads);                                  // distribute tileSeqLen among numThreads
        const ushort numTailThreads = numThreads * elemsPerThread - tileSeqLen;                         // last numTailThreads have one elem less
        const int threadTailId = (int) threadId - (numThreads - numTailThreads);                        // tail start indicated by ..., 0, 1, 2, ...
        const ushort threadSeqLen = (threadTailId < 0) ? elemsPerThread : (elemsPerThread-1);           // sequence length processed by every thread
        const ushort threadBaseIdx = threadId * elemsPerThread - max(threadTailId, 0);                  // base index to process by every thread

        //
        // Load inputs and coeffs of tile into thread-local arrays
        kT threadAccOutput[kMaxElemsPerThread];
        kT threadAccCoeff[kMaxElemsPerThread];
        for(ushort i = 0; i < kMaxElemsPerThread; i++) {
            threadAccOutput[i] = (i < threadSeqLen) ? inputs[tileBaseIdx + threadBaseIdx + i] : 0;  // load or fill with 0
            threadAccCoeff[i] = (i < threadSeqLen) ? coeffs[tileBaseIdx + threadBaseIdx + i] : 1;   // load or fill with 1
        }

        // Combine seqAccOutput and and -Gate with first threadAccOutput and -Gate 
        if (threadIdx.x == 0){
            threadAccOutput[0] = seqAccOutput * threadAccCoeff[0] + threadAccOutput[0];
            threadAccCoeff[0] =  seqAccCoeff * threadAccCoeff[0];
        } __syncthreads(); // avoid race condition

        _linrec_scan_tile_parallel_<kT, kMaxElemsPerThread kMaxThreadsPerWarp, kMaxThreadsPerBlock>(
            threadAccOutput, threadAccCoeff, numThreads
        );
    
        // Store last threadAccOutput and -Gate into seqAccOutput and and -Gate
        if (threadIdx.x == numThreads-1) {
            seqAccOutput = threadAccOutput[kMaxElemsPerThread-1];
            seqAccCoeff = threadAccCoeff[kMaxElemsPerThread-1];
        } __syncthreads(); // avoid race condition

        for(ushort i = 0; i < threadSeqLen; i++) { // Store outputs
            outputs[tileBaseIdx + threadBaseIdx + i] = threadAccOutput[i];
        }
    }
}
{% endhighlight %}
</details>

### Reveverse and Backward Implementation

From Part I we know that computing the backward pass through the linear recursion mainly consists of a reversed linear recursion and an index shift. We support the reverse recursion by loading and storing the tiles in reverse order into the thread-local arrays. For the tile layout, this means that we reverse the  `threadId` if the runtime-argument `rev` is true:

{% highlight cpp %}
const ushort threadIdrev = !rev ? threadIdx.x : (numThreads - threadIdx.x - 1);
{% endhighlight %}

Further, the access within each thread happen in reverse as well. We move this functionality into a separate `memio.h` file to keep our kernels nice and tidy:
{% highlight cpp %}
template <typename kT, typename count_t>
__forceinline__  __device__  void copy_naive(kT* __restrict__ dst, 
                                              const kT* __restrict__ src,  
                                              const count_t dstElemsPerThread, 
                                              const count_t srcElemsPerThread, 
                                              const bool rev, const kT fillval) {
    for (count_t i = 0; i < dstElemsPerThread; i++) {
        count_t j = !rev ? i : (srcElemsPerThread-1)-i;
        dst[i] = (i < srcElemsPerThread) ? src[j] : fillval;
    }
}
{% endhighlight %}

There, we also implement the logic to shift indices correctly between tiles and threads. Finally we experiment with different approaches to copying, such as vectorized and coalesced versions<d-footnote> for more information see <a href="https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/
">here</a> and <a href="https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/
">here</a>
</d-footnote>. The memory loading method is determined by the compile-time parameter `memcode`.

### Testing & Benchmarking

To get a better understanding of our implementations, we compile these configurations
<div class="l-body-outset">
{% highlight cpp %}
static constexpr auto CONFIG_NAMES = std::array{"kMaxElemsPerThread", "kMaxThreadsPerWarp", 
                                                "kMaxThreadsPerBlock", "memcode", "algocode"};
static constexpr auto CONFIG_LIST = product(std::array{4, 8, 16}, std::array{32}, 
                                                std::array{32, 64, 128, 256, 512, 1024}, 
                                                std::array{0, 1, 2}, std::array{0, 3});
{% endhighlight %}
</div>

One of the biggest pitfalls in writing CUDA kernels, occurs when intermediate variables are not stored in the register file but on the DRAM. This happens, when local arrays cannot be allocated statically or when the number of live variables exceed the number of registers. To find out more, we wrap the function `cudaFuncGetAttributes` to return meta data for all our kernels. Invoking python `eval/func_attrs.py --algocode 3` shows that `linrec_tile_fwd`, `linrec_tile_bwd`, and `linrec_pipe_fwd` make full use of the registers and only in a few configurations exhibit register spilling. On the other hand `linrec_pipe_fwd` has very high register pressure and which results in slight register spilling in some configurations.

To test and bechmark our all configurations, we invoke `python eval/tune.py --algocode 3` and observe that the errors are in the numerical regime. But which configurations yield the best performance for `linrec_pipe_fwd` and `linrec_pipe_bwd`? Surprisingly, throughput does not correlate with the degree of parallelism on a `A100-SXM4-80GB`. 

<div class="caption">
    Best performing configurations for `linrec_pipe_fwd`
</div>
<div class="l-body-outset">
<table>
    <tr>
        <td>seqlen</td>
        <td>16</td>
        <td>32</td>
        <td>64</td>
        <td>128</td>
        <td>256</td>
        <td>512</td>
        <td>1024</td>
        <td>2048</td>
        <td>4096</td>
        <td>8192</td>
        <td>16384</td>
        <td>32768</td>
        <td>65536</td>
    </tr>
    <tr>
        <td>kMaxElemsPerThread</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
    </tr>
    <tr>
        <td>kMaxThreadsPerBlock</td>
        <td>32.0</td>
        <td>32.0</td>
        <td>32.0</td>
        <td>32.0</td>
        <td>64.0</td>
        <td>128.0</td>
        <td>128.0</td>
        <td>512.0</td>
        <td>128.0</td>
        <td>256.0</td>
        <td>128.0</td>
        <td>128.0</td>
        <td>128.0</td>
    </tr>
    <tr>
        <td>memcode</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
    </tr>
    <tr>
        <td>throughput (GB/s)</td>
        <td>144.6</td>
        <td>289.3</td>
        <td>578.6</td>
        <td>952.9</td>
        <td>1200.0</td>
        <td>1378.7</td>
        <td>1542.9</td>
        <td>1600.0</td>
        <td>1630.2</td>
        <td>1643.1</td>
        <td>1621.3</td>
        <td>1605.0</td>
        <td>1575.7</td>
    </tr>
</table>
</div>

<div class="caption">
    Best performing configurations for `linrec_pipe_bwd`
</div>
<div class="l-body-outset">
<table>
    <tr>
        <td>seqlen</td>
        <td>16</td>
        <td>32</td>
        <td>64</td>
        <td>128</td>
        <td>256</td>
        <td>512</td>
        <td>1024</td>
        <td>2048</td>
        <td>4096</td>
        <td>8192</td>
        <td>16384</td>
        <td>32768</td>
        <td>65536</td>
    </tr>
    <tr>
        <td>kMaxElemsPerThread</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
        <td>4.0</td>
    </tr>
    <tr>
        <td>kMaxThreadsPerBlock</td>
        <td>32.0</td>
        <td>32.0</td>
        <td>32.0</td>
        <td>32.0</td>
        <td>64.0</td>
        <td>128.0</td>
        <td>128.0</td>
        <td>128.0</td>
        <td>256.0</td>
        <td>256.0</td>
        <td>256.0</td>
        <td>256.0</td>
        <td>256.0</td>
    </tr>
    <tr>
        <td>memcode</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
        <td>0.0</td>
    </tr>
    <tr>
        <td>throughput (GB/s)</td>
        <td>177.6</td>
        <td>355.3</td>
        <td>675.0</td>
        <td>1125.0</td>
        <td>1350.0</td>
        <td>1521.1</td>
        <td>1600.0</td>
        <td>1588.2</td>
        <td>1573.8</td>
        <td>1572.3</td>
        <td>1568.1</td>
        <td>1521.8</td>
        <td>1490.0</td>
    </tr>
</table>
</div>

Instead, if we look best performing configurations in the tables below, we notice that a `kMaxElemsPerThread=4`, `memcode=0` and `kMaxThreadsPerBlock~=128` is optimal. We therefore use this configuration as a default and compare with our PyTorch reference, a beta version of a higher-order associative scan in PyTorch<d-footnote> under development <a href="https://github.com/pytorch/pytorch/blob/main/torch/_higher_order_ops/associative_scan.py">here</a></d-footnote>, and our sequential reference in CUDA.   


## Discussion

We hope that this article lowers provides inspiration for further research on new sequence-mixing architectures.
